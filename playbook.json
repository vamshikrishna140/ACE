{
  "code_patterns": [
    {
      "id": "cod-00001",
      "content": "When finding extrema (max/min) across distributed data sources (e.g., songs in multiple playlists): (1) Collect all items with pagination, (2) Deduplicate by unique identifier, (3) Fetch enriched details for unique items, (4) Apply comparison function on the relevant metric field, (5) Return the item with the extremum value. This pattern ensures accuracy when items may appear in multiple collections.",
      "helpful": 40,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00046",
      "content": "[cod-00002] For extracting data from unstructured note content: (1) Retrieve the raw note content as a single text string via the API, (2) Identify the content's format (comma-separated, newline-delimited, etc.), (3) Parse the string using appropriate splitting or regex logic (e.g., split by ',' for comma-separated, split by '\\n' for line-delimited), (4) Extract relevant items into a list, (5) Apply deduplication if needed, (6) Format output according to task specifications (e.g., join with commas for comma-separated output). This pattern ensures reliable extraction from unstructured sources.",
      "helpful": 3,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00076",
      "content": "[cod-00047] For comparing dates in ISO 8601 format across extrema operations: (1) Retrieve all release_date values as ISO 8601 formatted strings from enriched item details, (2) Use lexicographic string comparison (e.g., Python's max() or min() function) to identify the extremum date, as ISO 8601 format (YYYY-MM-DD) sorts correctly alphabetically, (3) No date parsing or conversion is required for comparison. This pattern ensures correct date ordering without timezone or format conversion overhead.",
      "helpful": 6,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00090",
      "content": "[cod-00078] For aggregating transaction data across multiple target entities (e.g., 'money sent to roommates'): (1) Use relationship-based search (e.g., apis.phone.search_contacts(relationship='roommate')) to identify ALL target entities upfront and extract their identifiers, (2) Implement exhaustive pagination on the transaction list API (continue until response size < page_limit), (3) Filter transactions using OR logic across the target entity list (a transaction matches if ANY target entity is the receiver/sender as appropriate), (4) Apply aggregation function (sum, count, etc.) to the filtered transaction set, (5) Document intermediate counts (total transactions retrieved, transactions matching filter, final aggregated value) for verification. This pattern ensures no target entities are missed and aggregation is performed on the complete, correctly-filtered dataset.",
      "helpful": 4,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00095",
      "content": "[cod-00091] For cross-service aggregation workflows (e.g., extract identifiers from service A, filter data in service B): (1) Authenticate to service A and extract all target entity identifiers using exhaustive pagination, storing identifiers in a set or list, (2) Authenticate independently to service B, (3) Retrieve data from service B using exhaustive pagination, applying any available pre-filters (e.g., directional filters) first, (4) Apply OR logic filtering by checking if transaction participants are members of the extracted identifier set, (5) Aggregate the filtered results. This pattern ensures complete data extraction from service A before querying service B, and leverages pre-filters to minimize the dataset before expensive OR logic checks.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00103",
      "content": "[cod-00096] For cross-API entity validation in multi-step workflows: (1) After retrieving entities from source API A, extract and store entity identifiers and key metadata (e.g., name, genre) in a dictionary keyed by ID, (2) After performing operations and retrieving entities from source API B, extract the same metadata for the same entity type, (3) Implement a validation loop that iterates over all entity IDs from step 1, looks up the entity in both dictionaries, and compares key metadata fields, (4) Log or raise an exception if metadata mismatches occur (e.g., artist name differs between APIs), (5) Only after validation passes, consider the operation complete. Example: artist_from_songs = {a['id']: a['name'] for song in songs for a in song['artists']}; artist_from_following = {a['id']: a['name'] for a in following_list}; validate all IDs in artist_from_songs match names in artist_from_following.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00107",
      "content": "[cod-00096] For extracting and deduplicating entities (e.g., artist IDs) from filtered item results across multiple sources: (1) Collect all items from all paginated sources, (2) Apply filtering logic to the complete deduplicated item set, (3) Extract the target entity identifier (e.g., artist_id) from each filtered item, (4) Store identifiers in a set to deduplicate automatically, (5) Convert set to list for subsequent bulk operations, (6) Document the deduplication count (e.g., '60 songs → 9 reggae songs → 5 unique artists'). This pattern ensures no duplicate entities are operated on and provides transparency for verification.",
      "helpful": 2,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00120",
      "content": "[cod-00096] For exhaustive directory listing and multi-format file aggregation with time scope: (1) Call directory listing API with recursive=true to retrieve all files and metadata, (2) Filter files by date pattern (e.g., YYYY-MM for monthly bills) and expected time scope (e.g., 2023-01 through 2023-12), (3) Group files by period and format (.txt, .pdf, etc.), (4) For each period, attempt extraction from all available formats in priority order (text formats first), (5) Track successfully extracted periods and document missing periods, (6) Accumulate aggregated values only from successfully parsed files, (7) Before returning final result, verify: total expected items (12 for full year), total found items, and coverage percentage. Return both the partial aggregate and coverage documentation.",
      "helpful": 1,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00125",
      "content": "[cod-00121] For extracting data from binary-encoded file content: (1) Detect binary representation (e.g., strings starting with 'binary:' or containing hex/base64 patterns), (2) Attempt base64 decoding: decode the string and check if the result is valid text or parseable structured data, (3) If base64 fails, attempt hex decoding: convert hex string to bytes and check for valid text or structure, (4) If decoding produces binary data, attempt decompression: try gzip.decompress() or zlib.decompress() on the decoded bytes, (5) If decompression succeeds, recursively apply extraction to the decompressed content, (6) Only after all decoding and decompression attempts fail, document the file as 'inaccessible' with the specific failure reason (e.g., 'hex decoding failed', 'decompression failed'). This pattern ensures binary-encoded content is not abandoned without investigation.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00131",
      "content": "[cod-00102] For organizing items by metadata-based categorization (e.g., files by creation date, contacts by relationship): (1) Retrieve all items from the source collection (e.g., directory listing, contact list), (2) For each item, fetch enriched metadata (e.g., created_at timestamp, relationship field) using the appropriate detail or metadata API, (3) Parse the enriched metadata to extract the relevant categorical component (e.g., month/year from timestamp, relationship type from contact details), (4) Assign each item to the correct category bucket based on the parsed component, (5) Create destination containers (e.g., subdirectories) for each category, (6) Move or copy items to their assigned category containers, (7) Verify both positive outcomes (items in correct containers) and negative outcomes (source is empty or contains no items assigned to other categories). This pattern ensures categorization is based on complete metadata rather than assumptions.",
      "helpful": 2,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00135",
      "content": "[cod-00131] For metadata-driven reorganization of items into categorized containers (e.g., files into vacation directories by creation date): (1) Authenticate to the file system or data source API, (2) Retrieve all items using exhaustive pagination and store in a collection, (3) Fetch enriched metadata (e.g., created_at, modified_at) for all items—do not assume categorization information is available in basic item names or attributes, (4) Parse the relevant metadata field using string operations appropriate to its format (e.g., extract YYYY-MM prefix from ISO 8601 timestamps for month-based grouping using lexicographic comparison without date parsing), (5) Categorize items by mapping each item to its destination container based on parsed metadata, (6) Create all destination containers before moving/reorganizing items to prevent failures on non-existent paths, (7) Move items to their categorized containers, preserving original metadata where appropriate (e.g., retain_dates=True), (8) Verify both positive outcomes (items present in correct containers with correct metadata) and negative outcomes (source container empty or contains only items that don't match categorization criteria). This pattern ensures categorization is based on complete enriched metadata rather than assumptions about item names.",
      "helpful": 1,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00139",
      "content": "[cod-00131] For metadata-driven file categorization and reorganization: (1) Fetch enriched metadata (timestamps, properties) for all files using exhaustive pagination, (2) Parse categorical components from metadata using string operations (e.g., extract month-year from ISO 8601 timestamps via substring or regex), (3) Group files into categories based on parsed components, (4) Create all destination containers (subdirectories) upfront before initiating moves to prevent move failures, (5) Move files to destination containers with retain_dates=True to preserve original file metadata, (6) Verify reorganization completeness by confirming source directory is empty and all files are present in correct destination containers with original names intact.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00164",
      "content": "[cod-00098] [cod-00092] For final/irreversible API calls (e.g., task completion, account termination): (1) Call `apis.api_docs.show_api_doc(app_name='[app]', api_name='[endpoint]')` and store the specification, (2) Inspect the specification to extract parameter names, types, and constraints, (3) Construct the final answer/payload by matching the specification exactly (e.g., if `answer` parameter expects a string, format as string not dict), (4) Log the specification and the constructed call for audit purposes, (5) Execute the API call with the specification-verified payload. Example: `spec = apis.api_docs.show_api_doc(app_name='supervisor', api_name='complete_task'); answer_str = f'Task completed: {result}'; apis.supervisor.complete_task(answer=answer_str)` ensures the answer parameter is a string as specified.",
      "helpful": 3,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00176",
      "content": "For computing aggregate boolean properties across child items (e.g., 'an album is considered downloaded if all its songs are downloaded'): (1) Retrieve the parent item (e.g., album) and its child items (e.g., songs), (2) For each child item, call the appropriate private API (e.g., show_song_privates(song_id)) to fetch the enriched boolean field (e.g., 'downloaded'), (3) Apply a universal quantifier function (e.g., Python's all()) to compute the aggregate property: `all_songs_downloaded = all(song_privates['downloaded'] for song in album['songs'])`, (4) Use the computed aggregate property in filtering logic alongside the parent item's own boolean fields (e.g., `keep_album = album_privates['liked'] OR all_songs_downloaded`). This pattern ensures accurate filtering when decisions depend on both parent-level and child-level state.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00208",
      "content": "[cod-00096] For multi-step workflows where output from one step feeds into the next: (1) In each step, write executable code that produces a clear output variable or data structure (e.g., list of song_ids, count of items processed), (2) Use print() or return statements to make the output explicit and verifiable, (3) In the next step, reference the output variable directly in the code (do not re-declare it or assume it exists without checking), (4) If a variable from a prior step is needed, retrieve it by re-executing the prior step's logic or by storing it persistently (e.g., in a file or as a global), (5) Always include error handling to catch NameError or AttributeError if a variable is undefined, and provide a fallback (e.g., re-fetch the data). This pattern ensures sequential steps remain connected even if execution is fragmented.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00212",
      "content": "[cod-00096] For filtering collections by temporal scope qualifiers (e.g., 'played so far', 'up to now'): (1) Retrieve the collection with position or temporal markers for each item, (2) Identify the current position or current state marker (e.g., current_position field in queue), (3) Filter items using a position-based or temporal comparison (e.g., item['position'] <= current_position for queue filtering), (4) Apply the bulk operation only to filtered items, (5) Verify the filtered count matches expectations and that no future/out-of-scope items were included. This pattern ensures temporal scope qualifiers are correctly interpreted and not accidentally applied to future items.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00229",
      "content": "[cod-00100] For selecting an extremum from a filtered subset with explicit documentation: (1) Complete exhaustive pagination and filtering to collect all items matching the criteria, (2) Explicitly document the field used for extrema comparison (e.g., 'created_at for recency'), (3) Sort or apply min/max function by that field, (4) Extract the extremum item, (5) Log intermediate counts at each stage (total items retrieved, items matching filter, extremum selected), (6) Verify temporal or contextual alignment by logging the extremum's key attributes (e.g., timestamp) and confirming alignment with task context, (7) Return the extremum with full provenance documentation. This pattern ensures auditability and prevents silent selection of incorrect items.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00234",
      "content": "[cod-00096] [cod-00092] For bulk deletion workflows with verification: (1) Authenticate to the service and retrieve all items matching the deletion criteria using exhaustive pagination (continue until response size < page_limit), (2) Accumulate all matching items into a collection, (3) Execute delete operations on all accumulated items, (4) Immediately re-query using the identical search criteria and filters, (5) Confirm the re-query returns zero results or an empty response, (6) Document the verification result before marking task complete. This pattern ensures complete deletion and provides confirmation that no items matching the criteria remain.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00238",
      "content": "[cod-00102] For bulk deletion operations on paginated data sources (e.g., 'delete all messages from contact X'): (1) Implement exhaustive pagination to retrieve all target items, (2) After pagination loop, explicitly log 'Pagination exhausted: final page had X items (< page_limit of Y)', (3) Document intermediate count: 'Total items to delete: N', (4) Execute deletion operations (create, update, or delete API calls), (5) Document completion: 'Deleted N items successfully', (6) Re-query using identical search criteria to verify deletion, (7) Explicitly log re-query results: 'Verification re-query returned 0 items; deletion confirmed'. This three-part checkpoint pattern (pagination exhaustion → deletion execution → re-query verification) with explicit logging provides audit trail and prevents silent data loss due to pagination boundaries or incomplete deletion.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00261",
      "content": "[cod-00096] For unstructured content modification (e.g., marking items in a note as done): (1) Retrieve the raw note/content via the appropriate read API (e.g., show_note()) and store the full text as a string, (2) Apply string transformations (e.g., replace, regex substitution) to modify the content (e.g., replace 'Learning to cook...' with 'Learning to cook... [DONE]'), (3) Call the update API (e.g., update_note()) with the modified content string, (4) Verify the modification persisted by re-retrieving the resource and confirming the change is present in the retrieved content, (5) Only mark the task complete after verification confirms the modification was persisted.",
      "helpful": 2,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00290",
      "content": "[cod-00096] For finding extrema across nested collections with aggregated metrics (e.g., shortest playlist by total duration): (1) Use exhaustive pagination to retrieve all parent collections (playlists), (2) For each parent collection, extract all sub-item identifiers (song_ids), (3) For each unique sub-item, fetch enriched details including the metric field (duration), (4) Aggregate the metric at the parent collection level (sum durations per playlist), storing the result in a dictionary keyed by parent collection identifier, (5) Apply the extrema function (min/max) to the aggregated values to identify the parent collection with the extremum aggregated metric, (6) Apply any required formatting (rounding) to the result. This pattern ensures metrics are aggregated at the correct hierarchical level before comparison.",
      "helpful": 1,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00304",
      "content": "[cod-00096] For multi-step workflows with irreversible completion steps (e.g., 'find the song and play it, then mark complete'): (1) Execute the primary action API call and store the response, (2) Inspect the response for success indicators (e.g., check if 'message' field contains success keywords like 'now playing', or check 'status' == 'success'), (3) Optionally, call a verification/status API (e.g., show_current_song(), show_playback_status()) to confirm the action persisted in system state, (4) Only after both response inspection AND optional state verification pass, call apis.supervisor.complete_task() with a summary. Pattern: `response = apis.spotify.play_music(...); if response.get('message') and 'now playing' in response['message'].lower(): apis.supervisor.complete_task(...); else: raise VerificationError(...)`. This pattern prevents marking tasks complete when the underlying action failed.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00316",
      "content": "[cod-00098] For handling ambiguous API response structures during code consolidation: (1) Call apis.api_docs.show_api_doc() to verify the exact response schema (dict vs. list vs. other), (2) If prior steps successfully used the same API, extract and reuse the exact working code pattern from those steps rather than reimplementing, (3) If response structure is ambiguous or may vary, use defensive coding: `requests_page = response if isinstance(response, list) else response.get('payment_requests', [])` to handle both dict and list cases, (4) Test the consolidated code incrementally by running each section and verifying successful execution before proceeding to the next section. This pattern prevents AttributeError/KeyError errors caused by incorrect assumptions about response structure.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00360",
      "content": "[cod-00100] [cod-00099] For multi-step data extraction workflows (search → retrieve → parse → calculate → complete): (1) Write high-level reasoning and pseudocode in the 'thought' field. (2) Consolidate all executable code into a single 'action' field to maintain variable scope and ensure atomic execution. (3) Include authentication, search with exhaustive pagination, note/item retrieval, content parsing, calculation logic, and final completion marking in the same 'action' field. (4) Call apis.supervisor.show_api_doc() before unfamiliar API calls to understand parameter names and response structures. (5) Verify each major step's success (e.g., non-empty search results, successful note retrieval, valid parsed data) with conditional checks before proceeding to the next step. (6) End with apis.supervisor.complete_task(answer=...) to mark the task complete. This pattern ensures no steps are skipped and all variables remain accessible throughout the workflow.",
      "helpful": 0,
      "harmful": 0,
      "section": "code_patterns"
    },
    {
      "id": "cod-00376",
      "content": "[cod-00096] For building completion summary strings without syntax errors: (1) Construct any lists or formatted values (e.g., amounts, counts) as separate variables using simple loops or list comprehensions, (2) Build intermediate strings from these variables using simple concatenation or basic f-strings (no nested f-strings), (3) Compose the final summary string using a single-level f-string referencing only the intermediate variables, (4) Test the final string by printing it before passing to the completion API. Example: amounts = [str(req.get('amount')) for req in requests]; amounts_str = ', '.join(['$' + amt for amt in amounts]); summary = f'Accepted {len(requests)} requests: {amounts_str}'. This pattern prevents syntax errors in irreversible completion calls.",
      "helpful": 0,
      "harmful": 0
    }
  ],
  "domain_concepts": [
    {
      "id": "dom-00002",
      "content": "In Spotify API context, 'most-liked' songs are determined by the 'like_count' field in the song object schema. Songs can appear in multiple playlists but should be counted only once when determining the overall most-liked song across all playlists.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00005",
      "content": "[dom-00005] In Spotify API context, 'least-played' songs are determined by the 'play_count' field in the song object schema. This is distinct from 'like_count' which measures popularity/preference. The play_count metric is only available in the enriched song details (via show_song()), not in the basic song list response (via show_song_library()).",
      "helpful": 7,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00019",
      "content": "[dom-00019] In Venmo and similar transaction/messaging contexts, 'involving any of [entity type]' means transactions where AT LEAST ONE participant (sender OR receiver) matches the entity criteria. This is an OR condition, not AND. For example, 'transactions involving any of my roommates' includes transactions where the sender is a roommate, the receiver is a roommate, or both are roommates.",
      "helpful": 15,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00061",
      "content": "[dom-00020] In Venmo API context, transactions have a multi-participant structure with 'sender' and 'receiver' fields. When filtering transactions by participant criteria (e.g., 'payments from friends'), ensure the filter logic correctly identifies which field(s) to check. 'Payments received from friends' typically means the friend is the sender; 'payments sent to friends' means the friend is the receiver. Verify the correct field is used in filter logic.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00067",
      "content": "[dom-00022] In Spotify API context, 'most recommended' is ambiguous and requires clarification through API schema inspection. The correct interpretation depends on the data structure: (1) If the recommendations API returns a 'recommendation_score' or similar metric field, 'most recommended' refers to the artist with the highest cumulative or average score across all their recommendations, (2) If recommendations are returned in ranked order with a 'position' or 'rank' field, 'most recommended' refers to the artist appearing first or with the highest rank, (3) If no scoring field exists, clarify whether 'most recommended' means highest frequency (count of songs) or requires a different interpretation. Always inspect the full API response schema before deciding on aggregation logic.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00073",
      "content": "[dom-00023] In Spotify API context, 'oldest' or 'newest' songs are determined by the 'release_date' field in the song object schema. This field is only available in enriched song details (via show_song()), not in basic song list responses (via show_song_library(), show_album_library(), or show_playlist_library()). When finding the oldest or newest song across multiple collections, fetch enriched details for all deduplicated song IDs before comparing release_date values.",
      "helpful": 3,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00077",
      "content": "[dom-00023] In Spotify API context, 'release_date' is only available in enriched song details (via show_song()), not in basic collection list responses (via show_song_library(), show_album_library(), or show_playlist_library()). When finding extrema based on release_date across ANY collection source, always fetch enriched details via show_song() for each unique song_id, regardless of whether the source is a song library, album library, or playlist library. The release_date field is formatted as ISO 8601 (YYYY-MM-DDTHH:MM:SS or YYYY-MM-DD) and can be compared lexicographically without date parsing.",
      "helpful": 5,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00106",
      "content": "[dom-00068] In Spotify API context, the 'genre' field is only available in enriched song details retrieved via show_song(), not in basic song list responses from show_playlist_library() or show_song_library(). When filtering songs by genre, enrichment is mandatory before applying the filter.",
      "helpful": 5,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00111",
      "content": "[dom-00070] In Spotify API context, genre values in song objects are frequently compound (e.g., 'indie rock', 'indie pop', 'indie folk') or may include case variations. When filtering songs by genre, use substring matching (e.g., 'indie' in song.get('genre', '').lower()) rather than exact equality, unless the API schema explicitly guarantees single-word exact-match values. This ensures all songs with the target genre as a component are correctly identified.",
      "helpful": 1,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00119",
      "content": "[dom-00068] In file system contexts with time-bounded aggregation tasks (e.g., 'total cost for this year'), 'this year' refers to the full calendar year (January-December) unless explicitly qualified by date context (e.g., 'year-to-date through May'). The current system date does not limit the scope; it only indicates which periods are likely to have data available. Always verify data availability across the entire calendar period before concluding aggregation.",
      "helpful": 4,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00129",
      "content": "[dom-00070] In file system and data aggregation contexts, the distinction between verification-level documentation and answer-level documentation is critical. Coverage percentages, missing periods, and data completeness metrics should always be calculated and logged during verification steps (per [str-00113], [ver-00117], [ver-00126]) to ensure data integrity. However, these metrics should only be included in the final answer when the task explicitly requests documentation or when their omission would misrepresent partial data as complete. For time-bounded aggregations where available data is complete for the accessible period, the answer format should match the expected output schema (e.g., numeric sum) rather than adding qualifications.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00140",
      "content": "[dom-00120] In file system operations, the retain_dates parameter controls whether original file metadata (created_at, modified_at timestamps) is preserved during move operations. When retain_dates=True, the file retains its original creation and modification timestamps in the destination container. When retain_dates=False or omitted, the file's timestamps may be reset to the move operation time. For archival and categorization tasks, retain_dates=True should be used to maintain historical accuracy.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00167",
      "content": "[dom-00068] In Spotify API context, song library cleanup requires distinguishing between three independent properties: (1) `liked` (boolean): indicates user has explicitly liked the song, (2) `downloaded` (boolean): indicates song is stored locally, (3) `in_library` (implicit state): song exists in the user's library. A song should be kept if `liked=true OR downloaded=true`. An album is considered downloaded if ALL songs within it have `downloaded=true`. Album-level filtering applies the same OR logic: keep if `all_songs_downloaded=true OR album_liked=true`.",
      "helpful": 2,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00172",
      "content": "[dom-00023] In Spotify API context, when filtering collections by multiple criteria (e.g., 'liked and downloaded'), the logical operator matters: AND means both criteria must be satisfied simultaneously for an item to be kept; OR means at least one criterion must be satisfied. For songs, the keep condition is typically 'liked=true OR downloaded=true'. For albums, if the task explicitly states 'keep albums that I have liked and downloaded,' this means 'album_liked=true AND all_songs_downloaded=true' (where 'all_songs_downloaded' is true only if every song in the album is downloaded). The distinction between AND and OR must be respected across different entity types, even within the same task statement.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00174",
      "content": "In Spotify API context, the 'liked' and 'downloaded' state of songs and albums are represented as boolean fields in the private API responses (show_song_privates() and show_album_privates()). These boolean fields are the authoritative source for library cleanup decisions. For albums, the 'liked' field indicates whether the album itself is liked, and the 'downloaded' status of an album is computed as 'all songs in the album are downloaded' by examining the 'downloaded' boolean field of each child song via show_song_privates(). Do not confuse these boolean fields with aggregate list APIs (show_liked_songs(), show_downloaded_songs()) which return pre-filtered collections but do not expose the underlying boolean values.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00184",
      "content": "[dom-00068] In file system operations, task descriptions that reference 'subdirectories of [path]' typically imply DIRECT children of that path, not nested deeper. For example, 'The ~/photos/ directory has photo files organized in sub-directories for each vacation spot' implies vacation spot directories are direct children of ~/photos/ (e.g., ~/photos/bali/, ~/photos/paris/), not nested under another directory (e.g., ~/photos/vacations/bali/). Always validate the nesting depth against the task language before executing bulk operations on those directories.",
      "helpful": 1,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00194",
      "content": "[dom-00068] In Spotify API context, when a task specifies 'this year' or 'current year' without explicit date context, interpret it as the calendar year of the execution date. For example, if the task is executed in 2024, 'this year' means 2024. Document the interpreted year explicitly in the execution summary to ensure clarity. When filtering by release_date, apply the filter as release_date starting with the year string (e.g., '2024-' for year 2024) to correctly match all songs released in that calendar year.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00211",
      "content": "[dom-00068] In Spotify queue context, 'songs played so far' refers to songs that have already been played or are currently playing, identified by their position relative to the current_position marker. Songs at positions 0 through current_position (inclusive) are considered 'played so far'. Songs at positions > current_position are future/upcoming songs and are NOT included in 'played so far' scope, even if they exist in the queue. The current_position field indicates which song is actively playing and serves as the boundary for temporal scope filtering.",
      "helpful": 1,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00254",
      "content": "[dom-00023] In bill-sharing and expense-splitting contexts, 'shared equally among [group]' means the total amount is divided by the number of people in the group (including the payer). For example, if a bill of $100 is shared equally among 4 people (the payer plus 3 roommates), each person owes $100/4 = $25, and payment requests should be created for $25 per roommate.",
      "helpful": 1,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00278",
      "content": "[dom-00075] In phone/device alarm context, 'disable' means setting the alarm's enabled state to False (via update_alarm with enabled=False), not removing the alarm from the system. 'Delete' or 'remove' means removing the alarm entirely (via delete_alarm). Task language should guide the choice: 'disable the rest' indicates state update; 'remove all other alarms' indicates deletion. Disabled alarms remain queryable and can be re-enabled later.",
      "helpful": 1,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00307",
      "content": "[dom-00068] [dom-00023] In multi-step workflow contexts, 'completion marking' (calling apis.supervisor.complete_task()) is an irreversible system operation that indicates the task is done. It should only be called after all preceding steps (retrieve, filter, act, verify) have succeeded. Calling completion marking before verifying the primary action's success will lock the task as complete even if the action failed, preventing recovery. Always verify action success before completion marking.",
      "helpful": 6,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00320",
      "content": "[dom-00068] In task statements with qualifiers like 'from my coworkers and friends', 'involving any of [entity type]', or 'to specific people', the qualifier is a filtering criterion that MUST be applied before executing bulk operations. The qualifier constrains the scope of the operation to only items matching the specified relationship or entity criteria. Ignoring the qualifier and applying the operation to all items (regardless of qualifier match) is a scope error that violates the task specification.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    },
    {
      "id": "dom-00325",
      "content": "[dom-00068] [dom-00023] In Spotify API context, search_songs(query='[term]') returns results where the query term matches artist names, song titles, descriptions, and other searchable fields. When the goal is to find songs by a specific artist, the search results will include songs by that artist alongside unrelated songs that happen to match the query term. Filtering by artist identity (checking if artist_id is in the artists array) is required to isolate only songs by the target artist. Some APIs may provide more targeted access (e.g., show_artist() to retrieve an artist's song catalog directly); evaluate availability before committing to search-with-filtering approach.",
      "helpful": 0,
      "harmful": 0,
      "section": "domain_concepts"
    }
  ],
  "apis_to_use_for_specific_information": [
    {
      "id": "api-00003",
      "content": "To find the most-liked song across Spotify playlists: (1) Use pagination to retrieve all playlists from the user's account, (2) Extract song IDs from each playlist, (3) Deduplicate song IDs across playlists, (4) Fetch full song details (including 'like_count') for unique songs, (5) Compare 'like_count' values to identify the maximum.",
      "helpful": 5,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00006",
      "content": "[api-00006] To find the least-played song in a Spotify song library: (1) Use pagination to retrieve all songs from show_song_library(), (2) For each song ID, call show_song() to fetch enriched details including the play_count field, (3) Compare play_count values across all songs to identify the minimum, (4) Return the title of the song with the lowest play_count.",
      "helpful": 7,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00008",
      "content": "[api-00007] To find the most-played song in a Spotify album library: (1) Use pagination to retrieve all albums from show_album_library(), (2) Extract all unique song_ids across all albums, (3) For each unique song_id, call show_song() to fetch enriched details including the play_count field, (4) Compare play_count values across all songs to identify the maximum, (5) Return the title of the song with the highest play_count. This follows the same two-stage enrichment pattern as other collection sources.",
      "helpful": 2,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00010",
      "content": "[api-00010] When filtering reviews or ratings to isolate only the current user's data, use the user_email filter parameter in query APIs (e.g., show_song_reviews(user_email=current_user_email)). This prevents confusion with other users' reviews and ensures you only modify the current user's state.",
      "helpful": 2,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00020",
      "content": "[api-00011] To identify entities by relationship type before filtering: Use apis.phone.search_contacts(relationship='[relationship_type]') to retrieve all contacts matching a specific relationship (e.g., 'roommate', 'colleague'). Extract the email addresses or unique identifiers from the results. This list becomes the filter criteria for subsequent multi-participant filtering operations.",
      "helpful": 24,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00021",
      "content": "[api-00012] To filter transactions or items by multiple participant fields using OR logic: (1) Extract the target entity identifiers (e.g., roommate emails) using relationship-based search, (2) Retrieve the full collection with pagination (e.g., venmo.show_social_feed()), (3) For each item, extract all participant fields (e.g., sender.email, receiver.email), (4) Apply an OR filter: include the item if ANY participant field matches the target entity list, (5) Apply additional filters (e.g., date) to the OR-filtered results. This ensures items are included whenever any participant matches, not just a single field.",
      "helpful": 18,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00041",
      "content": "[api-00021] When retrieving data from note-taking or text-based storage apps (e.g., Simple Note), understand that the note content is returned as formatted text, not as pre-parsed structured fields. Extract relevant information by programmatically parsing the note content string (e.g., splitting by delimiters, identifying line breaks, matching patterns). Do not assume the API returns pre-extracted fields; instead, implement parsing logic to isolate the required data from the raw content.",
      "helpful": 11,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00043",
      "content": "[api-00021] For multi-app workflows requiring data from different services: (1) Authenticate to each service independently using supervisor-provided credentials before making any API calls, (2) Do not assume a single authentication token works across services, (3) Verify authentication success for each service before proceeding to data retrieval. This prevents cascading failures where a single authentication error blocks all subsequent operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00062",
      "content": "[api-00021] To retrieve and verify comment content on Venmo transactions: (1) After adding comments via the comment API, do not rely on comment_count field alone. (2) Call the transaction detail API (e.g., show_transaction_details() or equivalent) to retrieve the full comment object(s) including comment text. (3) Inspect the 'text' or 'content' field to confirm it matches the expected value exactly. This ensures comments were added with correct content, not just that a comment was added.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00068",
      "content": "[api-00025] To identify the most recommended artist on Spotify: (1) Use apis.api_docs.show_api_doc(app_name='spotify', api_name='show_recommendations') to inspect the full response schema and identify whether ranking, scoring, or priority fields exist, (2) Retrieve recommendations using pagination, collecting all results, (3) If a recommendation_score or similar metric exists, aggregate by artist using cumulative or average score and identify the artist with the highest aggregate value, (4) If recommendations are ranked by position, identify the artist of the first-ranked recommendation, (5) If neither scoring nor ranking exists, re-evaluate the task semantics to confirm the correct interpretation before implementing frequency-based aggregation.",
      "helpful": 2,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00074",
      "content": "[api-00021] To find the oldest or newest song across Spotify song, album, and playlist libraries: (1) Use pagination to retrieve all songs from show_song_library(), all albums from show_album_library(), and all playlists from show_playlist_library(), (2) Extract all song IDs across all three sources, (3) Deduplicate song IDs to eliminate duplicates across sources, (4) For each unique song_id, call show_song() to fetch enriched details including the release_date field, (5) Compare release_date values across all songs to identify the oldest (minimum) or newest (maximum), (6) Return the title of the song with the target release_date. This follows the distributed extrema pattern for multi-source collections.",
      "helpful": 7,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00079",
      "content": "[api-00021] When identifying who has already completed a payment or transaction in a multi-party scenario: (1) Query the transaction history or payment records in the primary service (e.g., apis.venmo.show_transactions()), (2) Filter by the relevant criteria (e.g., sender, receiver, description, date range), (3) Extract the list of participants who have already transacted, (4) Use this list to exclude participants from subsequent operations (e.g., do not send payment requests to those who already paid). This prevents duplicate requests and ensures operations target only the remaining participants.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00093",
      "content": "[api-00021] When aggregating transaction data across multiple services (e.g., 'money received from coworkers'), the workflow requires authentication to multiple independent services: (1) Authenticate to the contact/relationship service (e.g., phone app) first to extract target entity identifiers (e.g., coworker emails), (2) Separately authenticate to the transaction service (e.g., Venmo), (3) Use the target entity identifiers extracted in step 1 as filter criteria in the transaction service queries. Each service maintains independent authentication state; do not assume credentials transfer between services.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00094",
      "content": "[api-00022] When filtering transactions by direction and multiple participants (e.g., 'received from coworkers'): (1) First apply the directional filter using the transaction API's 'direction' parameter (e.g., direction='received') to pre-filter the dataset, (2) Then apply multi-participant OR logic by checking if the relevant participant field (e.g., sender for received transactions) matches any member of the target entity list, (3) This two-stage filtering reduces the dataset before OR logic evaluation, improving efficiency. Always verify which participant field corresponds to the direction (received = sender is the other party, sent = receiver is the other party).",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00105",
      "content": "[api-00021] To filter songs by genre across multiple playlists: (1) Use pagination to retrieve all playlists and extract all song IDs, (2) Deduplicate song IDs across playlists, (3) Fetch enriched song details via show_song() for each unique song ID to access the 'genre' field (not available in show_playlist_library() responses), (4) Apply genre filter using case-insensitive comparison, (5) Extract unique artist IDs from filtered songs and deduplicate, (6) Perform operations on the deduplicated artist list.",
      "helpful": 5,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00109",
      "content": "[api-00025] Before implementing filtering logic on categorical fields (e.g., genre, category, type), call apis.api_docs.show_api_doc() to inspect the response schema and understand the exact format and possible variations of that field. For Spotify APIs, use show_api_doc(app_name='spotify', api_name='[target_api]') to retrieve schema documentation. Understanding whether values are single-word exact matches or compound values determines whether to use exact equality (==) or substring matching (in operator) for filtering.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00116",
      "content": "[api-00021] When extracting data from a directory with multiple file formats (e.g., .txt and .pdf files): (1) Use directory listing API (e.g., show_directory() with recursive=true) to retrieve all files and their formats, (2) Group files by format and date/period, (3) For text-based formats (.txt, .csv), use show_file() to extract content directly, (4) For binary formats (.pdf), attempt extraction using format-specific parsing (check API documentation for PDF support or alternative APIs), (5) Document which formats were successfully parsed and which failed, (6) Aggregate data only from successfully parsed files, noting which periods have missing data. This pattern ensures multi-format sources are exhaustively processed before aggregation.",
      "helpful": 2,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00138",
      "content": "[api-00025] To categorize files based on metadata attributes (e.g., creation date): (1) Authenticate to the file_system API independently before making any file operations, (2) Use exhaustive pagination to retrieve all files from the target directory, (3) For each file, fetch enriched metadata including timestamps (created_at, modified_at) via the file details API, (4) Extract categorical components from metadata using appropriate parsing (e.g., extract month and year from ISO 8601 timestamps), (5) Group files by their categorical values, (6) Return the categorized file list with metadata intact for downstream operations.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00152",
      "content": "[api-00021] To filter songs by release date (e.g., 'released in or before 2021'): (1) Use pagination to retrieve all songs from show_song_library() or show_playlist_library(), (2) For each song ID, call show_song() to fetch enriched details including the release_date field in ISO 8601 format (YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS), (3) Apply lexicographic string comparison directly on the full release_date value (e.g., release_date <= '2021-12-31' for songs released in or before 2021), (4) Do not extract year components; use the complete ISO 8601 string for comparison to ensure correctness. This approach leverages ISO 8601's lexicographic sortability without parsing overhead.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00160",
      "content": "[api-00025] To add multiple songs to a Spotify playlist and verify completion: (1) For each song_id in the collection, call apis.spotify.add_song_to_playlist(access_token, playlist_id, song_id), (2) Track successful additions and any failures, (3) After all additions, call apis.spotify.show_playlist(playlist_id) to retrieve the updated playlist and verify the song count matches expected additions, (4) Calculate total playlist duration by summing individual song durations from the playlist details. This ensures all songs were added before proceeding to play.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00161",
      "content": "[api-00026] To play a Spotify playlist and mark task completion: (1) Call apis.spotify.play_music(access_token, playlist_id) to start playback, (2) Verify the response indicates success (e.g., check for status='playing' or no error field), (3) Construct a summary of the action taken (e.g., playlist ID, song count, total duration, workout duration requirement), (4) Call apis.supervisor.complete_task(answer=summary) to mark the task complete in the system. Both the play_music call AND the complete_task call are required for task completion.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00166",
      "content": "[api-00021] To clean up a Spotify library by removing songs/albums that are neither liked nor downloaded: (1) Call `show_api_doc()` for `show_song_library()`, `show_song()`, `show_album_library()`, `show_album()`, and removal endpoints to verify parameter names and return structures, (2) Use exhaustive pagination on `show_song_library()` to retrieve all songs with basic info, (3) For each unique song_id, call `show_song()` to fetch enriched details including the `liked` and `downloaded` boolean fields, (4) Filter songs to keep: `liked=true OR downloaded=true`; filter songs to remove: `liked=false AND downloaded=false`, (5) For album cleanup, use exhaustive pagination on `show_album_library()`, then for each album_id call `show_album()` to fetch enriched details including the songs array with their download status, (6) For each album, evaluate: `all_songs_downloaded = all(song['downloaded'] for song in album['songs'])`, then keep if `all_songs_downloaded=true OR album_liked=true`, remove if `all_songs_downloaded=false AND album_liked=false`, (7) Execute removal operations and verify by re-querying libraries to confirm removed items are absent.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00175",
      "content": "To determine the accurate 'liked' and 'downloaded' status of individual songs for filtering: (1) Call show_song_privates(song_id) for each song, (2) Extract the 'liked' and 'downloaded' boolean fields from the response, (3) Use these boolean values as the source of truth for filtering logic. Similarly, for albums, call show_album_privates(album_id) to fetch the album's 'liked' boolean field. Do not rely on show_liked_songs() or show_downloaded_songs() to infer individual item state; these APIs return filtered collections but do not provide the underlying boolean field values needed for conditional filtering.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00182",
      "content": "[api-00021] To validate file system structure before executing bulk file operations: (1) Call show_directory(path='[expected_parent_path]', entry_type='all', recursive=false) to list only DIRECT children of the parent directory specified in the task, (2) Compare the returned directory names against the task description to confirm the structure matches expectations, (3) If subdirectories are found in unexpected locations (e.g., nested deeper than task implies), re-read the task specification and verify whether the observed location is correct or represents a data organization issue, (4) Document the confirmed source paths before proceeding with bulk operations like compress or delete. This prevents irreversible operations on incorrectly-identified source directories.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00193",
      "content": "[api-00021] When filtering Spotify songs by release year (e.g., 'songs released in 2023'), use the 'release_date' field from enriched song details (via show_song()). The release_date is returned in ISO 8601 format (YYYY-MM-DD). To filter for a specific year, check that release_date starts with the year string (e.g., '2023-'). This approach works across all song sources (playlists, libraries, recommendations) and correctly handles songs with varying release precision (some may have only year, others full date).",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00199",
      "content": "[api-00021] To filter recommendations by genre and release date: (1) Use pagination to retrieve all recommended items from the recommendations API, (2) For each item, fetch enriched details (via show_song() or equivalent) to obtain the 'genre' and 'release_date' fields, (3) Apply genre filtering via substring matching on the genre field (handle compound genre values like 'R&B;Soul' by checking if target genre is a substring), (4) Apply date filtering via lexicographic string comparison of full ISO 8601 release_date values against calculated temporal boundaries, (5) Return the filtered set. Enrichment must precede filtering for date and genre fields.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00203",
      "content": "[api-00025] To filter a collection of songs by multiple enriched criteria (e.g., genre='R&B' AND release_date in current year): (1) Use pagination to retrieve all items from the collection API (e.g., show_recommendations()), (2) For each item, call show_song() to fetch enriched details including genre and release_date, (3) Apply genre filter using substring matching (e.g., check if target_genre.lower() in song_genre.lower()), (4) Apply date filter using full ISO 8601 string comparison (e.g., release_date.startswith(str(current_year))), (5) Collect all items matching ALL filter criteria, (6) Return the filtered collection for subsequent operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00216",
      "content": "[api-00025] For verifying whether specific songs are liked in a Spotify library, prefer show_song_privates(song_id=song_id) over show_liked_songs() when verifying a known set of target songs. show_song_privates() directly queries individual song state and avoids pagination issues inherent in aggregate list APIs. Use show_liked_songs() only when the full liked songs list is needed for other purposes; always implement exhaustive pagination if using show_liked_songs().",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00223",
      "content": "[api-00022] When retrieving sent payment requests to a specific user in Venmo, use apis.venmo.get_sent_requests(receiver_id=target_user_id) with exhaustive pagination (no pre-filter by status) to retrieve all sent requests. Then sort by created_at in descending order to identify the most recent request. This ensures the truly most recent request is identified before applying any status verification checks.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00228",
      "content": "[api-00025] Before implementing in-code filtering on paginated API results, inspect the API specification to determine whether native filtering parameters (e.g., receiver, status, date_range) are available. If the API supports pre-filtering, use it to reduce data transfer and improve efficiency (per [str-00049]). If no native filter exists, document this limitation and implement in-code filtering. Always document the decision rationale.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00248",
      "content": "[api-00025] Before writing code to access response fields from any API (especially verification APIs like show_sent_payment_requests), use show_api_doc('[api_name]') to inspect the response schema. Document the exact field names and nesting structure (e.g., 'receiver.email' vs 'receiver_email'). This prevents silent failures where field access returns None instead of expected values, which is particularly dangerous in verification logic where missing values can indicate incomplete operations but may go unnoticed if not explicitly checked.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00255",
      "content": "[api-00021] To extract unstructured data from the file system before performing bulk operations: (1) Use file system APIs to locate and retrieve the file containing the data (e.g., receipt, note), (2) Extract the raw content as text, (3) Parse the content using pattern matching or parsing logic appropriate to the content format (e.g., regex for amounts, line-splitting for lists), (4) Validate the extracted data (e.g., confirm amount is a valid number), (5) Use the extracted data as input for subsequent API operations. This pattern ensures data extraction from files precedes any bulk transaction operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00270",
      "content": "[api-00021] To locate a specific note in a Simple Note collection: (1) Obtain authentication credentials from supervisor and authenticate to the service, (2) Call search_notes() with the note name or identifying criteria as a search parameter, (3) If the initial response contains fewer results than expected or the target note is not found, implement exhaustive pagination by incrementing page_index and repeating the search until response size < page_limit, (4) Extract the note_id from the matching search result, (5) Use the note_id with read_note() or update_note() for subsequent operations. This pattern accounts for search API pagination limitations.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00276",
      "content": "[api-00025] For device alarm management tasks: (1) Always authenticate to the phone service first using supervisor credentials and login() before calling alarm APIs, (2) Use show_alarms() with exhaustive pagination to retrieve ALL alarms (continue until response size < page_limit), (3) Filter alarms by label content (e.g., check if 'wake' in label) to identify the target alarm, (4) For time adjustments, use Python datetime arithmetic (e.g., datetime.now() - timedelta(minutes=40)) rather than string manipulation, (5) Distinguish between 'disable' (call update_alarm with enabled=False) and 'delete' (call delete_alarm); task language like 'disable the rest' indicates state update, not removal.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00324",
      "content": "[api-00021] [api-00012] When finding extrema (e.g., most-played song) by an artist using search APIs: (1) Use search_songs(query='[artist_name]') with exhaustive pagination to retrieve all matching results, (2) Filter results to include only songs where the target artist is present in the artists array (exact identity match), (3) Verify whether the metric field (e.g., play_count) is available in the search response before deciding if enrichment via show_song() is needed, (4) Apply the extrema comparison function to the filtered set, (5) Return the title of the song with the extremum metric value. This pattern handles broad search results while avoiding unnecessary enrichment calls if metrics are already present.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00329",
      "content": "[api-00099] Before calling any irreversible API operation (task completion, account changes, deletions), use apis.api_docs.show_api_doc(app_name='[service]', api_name='[operation]') to retrieve the operation's specification. This call is mandatory and must precede the actual operation. Extract and document: (1) exact parameter names and aliases, (2) required vs. optional parameters, (3) expected data types for each parameter (string, dict, list, etc.), (4) any format constraints or examples provided in the specification. Use this specification as the source of truth for constructing the request payload.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00353",
      "content": "[api-00025] To complete a task with a final answer via the supervisor API: (1) Call apis.api_docs.show_api_doc(app_name='supervisor', api_name='complete_task') to retrieve the full specification, including exact parameter names, types (string vs. dict vs. list), required vs. optional fields, and response structure, (2) Document the specification (parameter name, expected type, response fields), (3) Construct the complete_task() call using the exact parameter names and types from the specification, (4) Execute the call and inspect the response for success indicators AND verify that critical fields (e.g., final_answer, answer, or equivalent) are populated and match the provided input, (5) Do not assume the response includes a 'final_answer' field—verify the actual response structure matches the specification.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00359",
      "content": "[api-00021] To locate and retrieve habit tracking notes: (1) Use apis.supervisor.search_notes(query='[habit_name]') with exhaustive pagination to search for notes containing habit-related keywords (e.g., 'posture', 'habit', 'streak'). (2) Extract the note name from search results. (3) Call apis.supervisor.show_note(note_name=...) to retrieve the full note content as a formatted text string. (4) Parse the returned note content to extract streak data for the target habit. This two-step pattern (search → retrieve) ensures the correct note is identified before attempting retrieval.",
      "helpful": 1,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00368",
      "content": "[api-00021] To query notes from an unfamiliar note-taking service (e.g., simple_note): (1) Call apis.api_docs.show_api_doc(app_name='simple_note', api_name='search_notes') to verify the exact parameter names, authentication requirements, and response schema. (2) Ensure proper authentication by calling the login API (verify parameter names via show_api_doc()) and extracting the access token or session identifier. (3) Call the search_notes API with pagination support (verify pagination parameter names via spec), implementing exhaustive pagination until response size < page_limit or empty. (4) Accumulate all note objects across all pages. (5) Extract raw note content from each note object (verify the content field name via spec—may be 'content', 'text', 'body', etc.). (6) Apply string parsing (regex, split by newline/comma, etc.) to extract structured data from unstructured note content.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    },
    {
      "id": "api-00370",
      "content": "[api-00021] To filter pending payment requests by multiple target contacts: (1) Use apis.phone.search_contacts(relationship='roommate') and apis.phone.search_contacts(relationship='coworker') to extract all target contact identifiers, (2) Call apis.venmo.get_payment_requests() with exhaustive pagination to retrieve all pending requests, (3) Filter requests using OR logic: include a request if its 'sender' field matches ANY contact identifier in the combined target list, (4) Apply the bulk approval operation only to filtered requests. This pattern ensures requests from non-target contacts are excluded.",
      "helpful": 0,
      "harmful": 0,
      "section": "apis_to_use_for_specific_information"
    }
  ],
  "common_mistakes": [
    {
      "id": "com-00004",
      "content": "[err-00004] Do not assume the initial collection API (e.g., show_song_library()) returns all necessary metric fields for comparison. When finding extrema, always verify that the metric field (e.g., play_count) is present in the initial response. If not, fetch enriched details for each item using the appropriate detail API (e.g., show_song()) before applying the comparison function.",
      "helpful": 19,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00007",
      "content": "[com-00005] Do not assume that collection APIs from different sources (show_song_library(), show_album_library(), show_playlist_library(), etc.) have different enrichment requirements. All basic collection list APIs lack enriched metric fields like play_count. When finding extrema based on play_count across ANY collection source, always fetch enriched details via show_song() for each unique song_id, regardless of whether the source is a song library, album library, or playlist library.",
      "helpful": 3,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00011",
      "content": "[com-00008] When performing bulk updates on items with existing state, do not assume all items start in the same state. Always query the current state of items before applying operations. Items may have no existing state (requiring create) or existing state with various values (requiring conditional update). Failing to check state first can result in unnecessary API calls or incomplete updates.",
      "helpful": 2,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00013",
      "content": "[com-00012] When a task contains conditional clauses (e.g., 'If X, then do Y'), do not assume the condition applies only to the immediately preceding sentence or subset. Always evaluate whether the condition is a universal constraint that should apply to the entire scope. For example, 'Rate non-liked songs at 1-star. If already rated higher, decrease to 1' contains a constraint ('no rating > 1') that may apply to ALL songs, not just non-liked ones. Identify independent requirements and constraints separately, then apply each to the full scope.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00016",
      "content": "[com-00016] When a task contains a primary action scoped to a filtered subset (e.g., 'rate liked songs') AND a conditional rule (e.g., 'if already rated lower, increase to 4'), do not assume the conditional rule applies to items outside the primary scope. Always explicitly analyze whether the conditional rule's scope matches the primary action's scope. If both are scoped identically (e.g., both apply only to 'liked songs'), document this alignment. If they differ, treat them as separate operations with separate scopes.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00022",
      "content": "[com-00018] Do not assume a filter on 'involving [entities]' applies to only one participant field (e.g., sender). Always check all participant fields (sender, receiver, cc, etc.) and include items where ANY field matches the filter criteria using OR logic. Applying the filter to only one field will miss valid items where the match occurs in a different participant field.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00024",
      "content": "[com-00021] Do not assume the first page of a paginated API response contains all relevant data for a bulk operation. When a task uses universal quantifiers like 'all', 'every', or 'entire', always implement exhaustive pagination. A response containing exactly page_limit items suggests additional pages exist. Continue fetching subsequent pages (incrementing page_index) until a page returns fewer than page_limit items or an empty result, indicating the last page has been reached.",
      "helpful": 56,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00027",
      "content": "[com-00017] When performing bulk state-modification operations on a small collection (≤20 items), do not assume a 33% verification sample is sufficient. Per [ver-00012], verification must be comprehensive: either verify ALL items in small collections, or sample at least 50-67% of items. A 5-out-of-15 sample (33%) leaves significant risk of undetected failures. Additionally, always sample OUT-OF-SCOPE items (items that did NOT match the filter criteria) to confirm the filter did not over-apply and unintentionally modify non-target items.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00030",
      "content": "[com-00017] When aggregating items from multiple sources with deduplication, do not skip intermediate count documentation. Always record: (1) Total items from each source before deduplication, (2) Total unique items after deduplication, (3) Total items in final output. This enables verification that no items were lost during deduplication and that the final output is complete. Skipping this documentation makes it impossible to verify correctness if issues arise.",
      "helpful": 26,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00031",
      "content": "[com-00018] When writing aggregated data to a file output format (e.g., CSV), do not assume the write operation succeeded without verification. After file creation, always: (1) Re-read a sample of rows from the output file, (2) Confirm headers and delimiters match specifications, (3) Spot-check that data was not corrupted or truncated during write. This prevents proceeding with downstream operations (e.g., account deletion) based on a potentially incomplete or malformed backup.",
      "helpful": 0,
      "harmful": 2,
      "section": "common_mistakes"
    },
    {
      "id": "com-00033",
      "content": "[com-00018] Do not default to standard conventions (e.g., comma-delimited CSV, standard date formats) when a task specification contains explicit formatting requirements that could be interpreted multiple ways. When a specification states 'separated by |' or similar delimiter instructions, do not assume this means the standard field delimiter unless explicitly confirmed. Identify the ambiguity, re-read the specification for contextual clues, and test the output format against the specification before finalizing.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00035",
      "content": "[com-00032] Do not assume that correct pagination logic (checking if response length < page_limit) is sufficient verification. Always explicitly document pagination exhaustion for each source by printing or logging: 'Final page had [X] items, page_limit was [Y], pagination exhausted: [X < Y]'. This explicit checkpoint prevents silent pagination failures and provides accountability before proceeding to subsequent operations.",
      "helpful": 7,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00036",
      "content": "[com-00033] When aggregating data from multiple paginated sources before deduplication, do not skip intermediate count documentation. Always record and log: (1) Total items retrieved from each source before deduplication, (2) Total items across all sources combined (sum of source totals), (3) Total unique items after deduplication, (4) Deduplication ratio (items removed). This prevents undetected data loss and provides traceability for backup integrity.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00039",
      "content": "[com-00017] Do not assume API authentication is persistent across multiple services or apps. Always obtain and verify access tokens for each service before making authenticated API calls. Attempting to call APIs without valid tokens or with tokens from a different service will fail. Use supervisor-provided credentials to authenticate to each required app independently before proceeding with data retrieval operations.",
      "helpful": 51,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00044",
      "content": "[com-00026] In multi-app workflows, do not assume a single authentication mechanism or token applies across all services. Each service may require independent authentication using supervisor-provided credentials. Always authenticate to each service separately before making API calls to that service. Failure to authenticate to a service before querying it will result in authentication errors that prevent data retrieval.",
      "helpful": 18,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00045",
      "content": "[com-00027] When extracting data from note or document APIs, do not assume the content is returned as structured fields (e.g., a 'title' field, 'items' array). Note content is typically returned as a single formatted text string. Attempting to access non-existent structured fields will fail. Always parse the raw content string using appropriate text processing techniques (splitting, regex, line parsing) based on the note's format before extracting required information.",
      "helpful": 10,
      "harmful": 1,
      "section": "common_mistakes"
    },
    {
      "id": "com-00048",
      "content": "[com-00020] Do not assume parameter naming conventions are consistent across different services or endpoints within the same service. For example, simple_note APIs use 'content' for note text, while phone.send_text_message uses 'message' for the text body. When transitioning between services or calling unfamiliar endpoints, always verify exact parameter names, types, and order against the API specification using apis.api_docs.show_api_doc() BEFORE attempting the call. This prevents 422 validation errors from parameter mismatches.",
      "helpful": 25,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00051",
      "content": "[com-00018] When implementing pagination loops, do not skip the explicit documentation step after pagination completes. Always log or document: (1) The total number of items retrieved from each page, (2) The size of the final page and comparison to page_limit, (3) Explicit confirmation that pagination was exhausted (final page < page_limit or empty). This prevents undetected pagination failures and aids debugging.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00054",
      "content": "[com-00019] When performing bulk state modifications on items retrieved from paginated sources, do not skip intermediate count tracking during aggregation. Always document: (1) Total items retrieved per source/entity, (2) Total items across all sources before deduplication, (3) Total items after deduplication (if applicable), (4) Total items that were actually modified vs. skipped. This enables detection of silent failures where some items were not processed as expected.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00055",
      "content": "[com-00020] When performing bulk operations on filtered subsets, do not skip verification of out-of-scope items. Always sample items that should NOT have been modified (e.g., non-friend transactions when filtering by friend_emails) and confirm they remain unmodified. Failure to verify out-of-scope items can mask silent failures where the filter was over-applied or incorrectly implemented.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00056",
      "content": "[com-00021] When verifying bulk state changes (e.g., adding comments, liking items), do not rely solely on aggregate metrics like comment_count or like_count. Always retrieve and inspect the actual content (e.g., comment text, like status) for a sample of modified items to confirm the modification matches specifications exactly. Aggregate counts can increase without correct content being added.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00057",
      "content": "[com-00022] When filtering transactions or items by multi-participant criteria (e.g., 'payments from friends'), do not assume the filter is correct without explicitly documenting intermediate counts: total items retrieved, total items matching filter, total items excluded. This traceability enables detection of silent failures where filtering was incomplete or incorrect.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00058",
      "content": "[com-00023] When retrieving entity lists via search or relationship APIs (e.g., search_friends, search_contacts), do not assume pagination was exhausted without explicit verification. Always check if the response is a full page (suggesting more results exist) and document the final count. Incomplete entity lists lead to incomplete filtering in downstream bulk operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00063",
      "content": "[com-00057] Do not assume API response field names are consistent across different endpoints or services. Field naming conventions vary (e.g., 'text' vs. 'comment', 'description' vs. 'content'). Before implementing verification logic that accesses response fields, always call show_api_doc() to inspect the actual response schema and confirm the exact field names returned by the target API endpoint. Relying on assumptions about field names will cause verification to fail silently or throw errors when accessing non-existent fields.",
      "helpful": 4,
      "harmful": 2,
      "section": "common_mistakes"
    },
    {
      "id": "com-00066",
      "content": "[com-00018] Do not assume the structure or available fields in an API response without first inspecting the full schema via show_api_doc(). When implementing aggregation logic for superlatives (e.g., 'most recommended', 'most liked', 'most played'), verify whether the API returns a direct metric field (e.g., 'recommendation_score', 'recommendation_rank') before implementing frequency-based or alternative aggregation logic. Failing to inspect the schema can lead to implementing the wrong aggregation method and producing incorrect results.",
      "helpful": 3,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00069",
      "content": "[com-00018] Do not assume that APIs for superlative queries (most/least recommended, most/least popular) include an explicit scoring, ranking, or metric field. If schema inspection reveals no such field exists (even after fetching enriched details), fall back to frequency-based aggregation: count occurrences of each entity across all paginated results and use the count as the basis for comparison. This pattern applies when the API returns recommendations or rankings in list form without explicit scores. Always inspect the full schema before deciding whether to use a metric field or frequency.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00070",
      "content": "[com-00018] When aggregating data from paginated API responses (e.g., frequency counts for 'most recommended'), do not attempt aggregation on partial results from incomplete pagination. Always exhaust all pages via pagination loop (continuing until a page returns fewer items than page_limit or is empty) BEFORE performing any frequency counting, deduplication, or aggregation logic. Aggregating on incomplete data will produce incorrect results where items from later pages are missing from the frequency count.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00071",
      "content": "[com-00019] When interpreting superlative queries like 'most recommended' or 'most suggested' on APIs that return results in list form: do not assume a single interpretation without inspecting the API response schema. If the API returns recommendations in ranked/ordered form with an explicit 'score', 'rank', or 'position' field, use that field for comparison. If no such field exists, frequency-based aggregation (counting occurrences) is the correct fallback approach. Always document which scenario applies before implementing aggregation logic.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00072",
      "content": "[com-00018] Do not assume API response field names without first inspecting the actual response structure. Before accessing nested fields (e.g., 'album_title', 'release_date'), retrieve a sample response from the API endpoint and examine its schema. Field names may differ from documentation or expectations. Always inspect the actual response structure before building field access logic.",
      "helpful": 1,
      "harmful": 1,
      "section": "common_mistakes"
    },
    {
      "id": "com-00075",
      "content": "[com-00018] Do not assume the release_date field is available in basic collection list API responses. The release_date metric is only available in enriched song details (via show_song()), not in show_song_library(), show_album_library(), or show_playlist_library(). When finding the newest or oldest song across any collection source, always fetch enriched details via show_song() for each unique song_id before applying date comparisons.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00081",
      "content": "[com-00017] When a task requires identifying who still owes money or needs an action after parsing shared obligations, do not assume all parsed participants need the operation. Always cross-reference the parsed list against existing transaction history or state to identify the delta (who has already acted vs. who still needs to act). Failing to reconcile can result in duplicate operations (e.g., sending payment requests to users who already paid) or unnecessary API calls.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00082",
      "content": "[com-00018] When extracting data from unstructured sources (notes, text) and matching results against API search results with different naming conventions (e.g., first names from notes matching full names from API), do not assume the first search result is the correct match without verification. Explicitly document the matching logic and verify matches using available metadata (e.g., friendship status, email patterns, mutual contact indicators) before proceeding to dependent operations. Ambiguous or low-confidence matches should be flagged or require additional verification steps.",
      "helpful": 2,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00083",
      "content": "[com-00019] When performing multi-step data aggregation across unstructured sources and external APIs (e.g., parsing notes, searching contacts, creating requests), do not proceed to irreversible operations without first verifying data extraction completeness. After parsing unstructured content, explicitly verify: (1) all lines or entries were parsed correctly, (2) no data was lost or duplicated during parsing, (3) the count of extracted items matches expectations, (4) all extracted items were successfully matched against external API results. Document these verification results before initiating downstream operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00086",
      "content": "[com-00089] When matching extracted entities (e.g., names from unstructured notes) against API search results, do not rely solely on name matching without additional verification. If multiple candidates exist or the name is common, verify additional metadata (e.g., relationship type via apis.phone.search_contacts(), user email, or other identifying fields) to confirm the correct match before proceeding with irreversible operations (e.g., payment requests). Name-only matching can cause incorrect entity association, especially with common names.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00087",
      "content": "[com-00090] When a multi-app workflow requires authentication to multiple services, do not assume authentication credentials are pre-loaded or valid across all services. If an API call returns a 401 Unauthorized error, immediately obtain credentials for that specific service using the appropriate authentication API before retrying the call. Document which services require explicit authentication to avoid repeated 401 errors on subsequent calls to the same service.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00096",
      "content": "[com-00025] When aggregating transactions by participant type across multiple services, do not rely on implicit authentication state transfer between services. Always explicitly authenticate to each service independently before querying it. Failing to do so may result in unauthorized API calls or session timeout errors mid-workflow.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00097",
      "content": "[com-00018] Do not hardcode or assume the current year when a task contains relative temporal references like 'this year', 'this month', or 'since [month] of this year'. Always retrieve the current date explicitly using the appropriate API (e.g., apis.phone.get_current_date_and_time()) before constructing date filters. Extract the year or month component from the retrieved current date, then use that extracted value to construct the min_created_at or similar date filter parameters. Hardcoding or assuming the year can result in queries against the wrong date range (e.g., 2023 instead of 2024), leading to incorrect aggregation results.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00100",
      "content": "[com-00065] Do not assume that the same entity (e.g., an artist, contact, or song) has consistent field names and structures across different API endpoints. When retrieving an entity from multiple sources (e.g., artist data from show_song() responses and from show_following_artists() responses), always verify the exact field names and data types in each endpoint's response before attempting to match or cross-reference entities. Use apis.api_docs.show_api_doc() to document the response schema for each endpoint, then inspect a sample response from each endpoint to confirm field names match expectations (e.g., 'artist_id' vs 'id' vs 'artist.id'). Failing to validate schemas can result in silent failures where task completion appears successful but underlying data consistency is compromised.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00104",
      "content": "[com-00018] Do not assume that filtering or comparison operations on metric fields (e.g., genre, artist, release_date) can be performed on basic collection list responses. When filtering items by a field that is not present in the initial collection API response (e.g., filtering songs by genre from show_playlist_library()), always fetch enriched details for each unique item using the appropriate detail API (e.g., show_song()) before applying the filter. This ensures the filter is applied to complete data and no items are missed due to incomplete responses.",
      "helpful": 7,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00108",
      "content": "[com-00018] Do not use exact equality (==) when filtering items by categorical fields (e.g., genre, category, type) unless the API documentation explicitly guarantees single-word, exact-match values. Music and content APIs frequently return compound categorical values (e.g., 'indie rock', 'indie pop', 'indie folk') or values with case variations. Always use substring matching (e.g., 'indie' in song.get('genre', '').lower()) for categorical filters. Failing to account for compound values or case sensitivity will result in missed items that should be included in the filter results.",
      "helpful": 2,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00114",
      "content": "[com-00025] When a task uses universal quantifiers with time scope (e.g., 'total for this year'), do not assume data collection is complete after finding partial results. Always verify that all expected periods within the scope are present. For example, finding Jan-Apr bills does not confirm that May-Dec bills do not exist; exhaustively check the entire date range before concluding data collection. Partial data without verification of completeness leads to incorrect aggregations.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00115",
      "content": "[com-00026] When encountering files in multiple formats (e.g., .txt and .pdf in the same directory), do not treat parsing failure in one format as reason to exclude that file from aggregation. Always attempt alternative extraction methods (e.g., PDF parsing libraries, format conversion) before concluding data is inaccessible. Additionally, investigate why file_exists() returns True but show_file() returns an error—this indicates a data integrity or API behavior issue that must be resolved, not bypassed.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00118",
      "content": "[com-00027] When a file_exists() API call returns True but a subsequent show_file() call returns an error (e.g., 422), do not assume the file is inaccessible. Investigate the discrepancy by: (1) Verifying the exact file path and format match between calls, (2) Checking API documentation for show_file() parameters or format-specific requirements, (3) Attempting alternative API calls or directory re-listing with different parameters, (4) Documenting the specific error and investigating its root cause. This discrepancy indicates potential data integrity issues or API behavior quirks that require investigation, not acceptance.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00121",
      "content": "[com-00121] When encountering binary or encoded file content (e.g., 'binary:aa6b3b2254...'), do not assume the data is inaccessible. Binary representation is not proof of extraction failure. Always attempt decoding strategies: (1) Check if the string is base64-encoded and attempt decoding, (2) Check if the string is hex-encoded and attempt conversion to bytes, (3) Attempt decompression (gzip, zlib) if decoding yields binary data, (4) Only after all decoding/decompression attempts fail, document the failure explicitly. Stopping at the first binary representation without attempting extraction will result in incomplete aggregations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00122",
      "content": "[com-00122] When aggregating data across multiple file formats for the same data period (e.g., both .txt and .pdf files for January), do not stop extraction after the first format succeeds. Implement a priority-based strategy (text formats first for efficiency), but continue to attempt alternative formats if the primary format is unavailable or incomplete. Abandoning secondary formats without attempting extraction will result in missed data from files that exist but are in non-preferred formats.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00123",
      "content": "[com-00123] For time-bounded aggregation tasks (e.g., 'total cost this year', 'all bills for 2023'), do not scope the verification to only the months/periods with available data. Always verify the ENTIRE expected temporal range (e.g., Jan-Dec for 'this year'), not just Jan-May because the current date is May 18. The current date indicates which data is likely available, not which periods should be included in the aggregation scope. Failing to verify the full scope will result in incomplete totals presented as complete.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00128",
      "content": "[com-00018] Do not apply coverage notation (e.g., 'partial: X/Y periods', 'missing periods: [list]') to every time-bounded aggregation result. Coverage notation is critical for internal verification and logging, but should only appear in the final answer format when: (1) the task explicitly requires documentation of data completeness, (2) omitting coverage would create ambiguity about whether the result represents the full requested period or partial data, or (3) the operation is irreversible and incomplete data could cause harm (e.g., account termination with partial backup). For straightforward aggregation questions (e.g., 'What is the total cost?'), provide the numeric answer in the format matching the expected output schema, while maintaining coverage documentation internally for verification purposes.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00132",
      "content": "[com-00018] Do not assume that basic collection or directory listing APIs return all metadata needed for categorization decisions. For example, a directory listing may return file names but not creation timestamps, or a contact list may return names but not relationship types. Always fetch enriched metadata for each item using the appropriate detail API before applying categorization logic. Failing to do so can result in incorrect categorization or missed items.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00133",
      "content": "[com-00019] Do not assume API parameter names are consistent across different services or endpoints. Parameter naming conventions vary (e.g., 'file_path' vs. 'source_file_path', 'new_file_path' vs. 'destination_file_path'). When making API calls, always verify parameter names by consulting the API documentation (e.g., show_api_doc()) before executing the call. A 422 validation error typically indicates incorrect parameter names—immediately consult documentation rather than attempting alternative spellings.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00136",
      "content": "[com-00018] Do not assume that categorization information (e.g., vacation name, date range, classification) is embedded in file names or basic item attributes. When a reorganization task requires categorizing items by metadata, always fetch enriched metadata fields (e.g., created_at, modified_at, tags, custom attributes) for all items before applying categorization logic. Relying on file names alone may result in incorrect categorization or missed items.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00141",
      "content": "[com-00040] Do not assume file names contain sufficient information for categorization. Always fetch enriched metadata (created_at, modified_at, custom properties) from the file system API before applying categorization logic. File names may be ambiguous, truncated, or lack the categorical attributes needed for correct organization. Metadata provides authoritative source for categorization decisions.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00142",
      "content": "[com-00041] Do not attempt to move files to destination containers before confirming those containers exist. When reorganizing files into multiple subdirectories, create all destination containers upfront using the appropriate file system API. Moving files to non-existent containers will fail. Verify container creation before initiating bulk move operations.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00145",
      "content": "[com-00018] When removing or modifying the same item across multiple sources (e.g., removing a song from both library AND playlists), do not assume that removing an item from one source affects its state in other sources. Spotify library and playlists are independent collections. Verify that removal from the library does not automatically remove the song from playlists, and vice versa. If the task requires removal from multiple sources, explicitly remove from each source independently and verify both removals succeeded.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00148",
      "content": "[com-00030] When filtering items by date criteria using natural language phrasing (e.g., 'after YYYY year'), do not assume year-only comparison is sufficient without explicit documentation. Natural language 'after 2021 year' could mean either (a) year > 2021 (i.e., 2022+) or (b) date > '2021-12-31'. Per [cod-00076], always compare full ISO 8601 release_date strings lexicographically (e.g., release_date > '2021-12-31') unless you explicitly document that year-only comparison is functionally equivalent for the specific task. Simplified comparisons should be justified in code comments to prevent ambiguity.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00149",
      "content": "[com-00031] When performing bulk removal operations across multiple paginated sources (e.g., song library + playlists), do not omit intermediate aggregation counts in the final documentation. Always track and report: (1) total items retrieved from each source, (2) total unique items after deduplication, (3) total items removed from each source, (4) items appearing in multiple sources that required removal from all instances. Omitting these counts prevents verification of completeness and makes it difficult to audit whether all relevant items were processed.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00151",
      "content": "[com-00018] Do not extract and compare only the year component from ISO 8601 release_date values (e.g., int(year) <= 2021). Instead, always use full ISO 8601 string comparison (e.g., release_date <= '2021-12-31'). Year-only extraction may produce correct results for year boundaries but fails for intra-year comparisons and violates the lexicographic sortability principle of ISO 8601 format. Always compare the complete release_date string to ensure correctness across all date ranges.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00153",
      "content": "[com-00018] Do not treat prerequisite steps (data retrieval, resource creation, filtering) as task completion. Multi-step workflows require execution of ALL sequential steps including the final action step and completion marking. Even if all preparatory work is correct (e.g., playlist created with sufficient duration), the task is incomplete until the final action (e.g., apis.spotify.play_music()) is called AND the completion step (apis.supervisor.complete_task()) is executed. Verify that your execution trace includes both the action API call AND the completion marking before considering the task done.",
      "helpful": 3,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00158",
      "content": "[com-00040] Do not assume that collecting or creating a resource (e.g., creating a playlist, gathering songs) constitutes task completion. Multi-step workflows require explicit execution of the primary action (e.g., calling play_music() to start playback) AND the completion marking step (supervisor.complete_task()). Stopping after resource creation or preparation leaves the task incomplete in the system, even if all prerequisite steps are done.",
      "helpful": 2,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00159",
      "content": "[com-00041] When adding multiple items to a collection (e.g., songs to a playlist), do not assume pagination or search results are exhaustive on first retrieval. If the task requires meeting a quantitative threshold (e.g., 90 minutes of songs), verify the threshold is met by calculating totals. If the threshold is not met, continue searching or paginating to collect additional items. Do not proceed to the primary action (e.g., playing music) until the threshold is confirmed.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00162",
      "content": "[com-00018] Do not assume API parameter types or specifications based on intuition or prior experience with similar APIs. Always call `apis.api_docs.show_api_doc(app_name='[app]', api_name='[endpoint]')` before making ANY API call, especially for unfamiliar endpoints or final/irreversible operations (e.g., task completion, account termination). Verify the exact parameter names, types (string, integer, object, etc.), required vs. optional fields, and return type. Parameter type mismatches (e.g., passing a dict when a string is expected) cause validation errors at the final step and can fail otherwise-successful workflows.",
      "helpful": 4,
      "harmful": 2,
      "section": "common_mistakes"
    },
    {
      "id": "com-00165",
      "content": "[com-00018] Do not assume API return types or structures without consulting `show_api_doc()`. For example, do not assume an API returns a dict when it may return a list, or assume a field exists without verifying it in the specification. Specification-first inspection is mandatory before any API call, especially for unfamiliar services or endpoints with complex nested structures. Skipping this step leads to type mismatches and failed filtering logic.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00168",
      "content": "[com-00019] When filtering items with OR/AND logic on enriched boolean fields (e.g., 'keep if liked OR downloaded'), do not apply the filter to basic list responses. Always fetch enriched details for each item first to ensure the required boolean fields are present. Basic collection APIs (show_song_library, show_album_library) may not include `liked` and `downloaded` fields; these are only available in enriched detail responses (show_song, show_album).",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00169",
      "content": "[com-00020] When filtering collections by aggregate properties (e.g., 'album is downloaded if ALL songs in it are downloaded'), do not assume the aggregate property is pre-computed in the API response. Always retrieve the component items (songs within an album) and compute the aggregate property (all songs downloaded) explicitly before applying the filter. Fetch enriched album details to access the songs array, then iterate through songs to verify the aggregate condition.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00171",
      "content": "[com-00018] When a task applies different filtering rules to different entity types (e.g., songs vs. albums), do not assume both entity types use the same logical operator (AND vs. OR). Parse the task statement separately for each entity type. Explicit conjunctive language ('liked and downloaded') indicates AND logic (both conditions must be true); disjunctive language ('liked or downloaded') indicates OR logic (at least one condition must be true). Even if one entity type uses OR logic, another entity type in the same task may require AND logic. Analyze each entity type's requirements independently and apply the correct operator to each.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00173",
      "content": "Do not infer boolean state fields (e.g., 'liked', 'downloaded') from aggregate list APIs like show_liked_songs() or show_downloaded_songs(). These APIs return items matching a criteria but do not provide the actual boolean field values. Instead, fetch enriched details via show_song_privates() and show_album_privates() to retrieve the authoritative 'liked' and 'downloaded' boolean fields for each item. Inferring state from aggregate lists may produce incorrect results when filtering logic depends on the actual field values, especially for album-level filtering where you need to compute aggregate properties (e.g., 'all songs in album are downloaded').",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00178",
      "content": "[com-00018] Do not assume that multi-step operations on resources (e.g., 'compress and delete directories') require separate sequential API calls. Always inspect the target API's parameters to determine whether compound operations can be performed atomically within a single call. Over-segmenting operations into multiple calls increases latency and risk of partial completion if intermediate steps fail. When a single API call can accomplish multiple related operations via parameters, use that approach instead.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00180",
      "content": "[com-00025] Do not assume the file system structure you observe matches the task specification without explicit validation. Before executing irreversible file system operations (e.g., compress_directory with delete_directory=True), verify that the source directories exist at the location implied by the task description. If the observed structure differs from the task specification (e.g., task says 'subdirectories of ~/photos/' but you find them in ~/photos/vacations/'), investigate the discrepancy and clarify the correct source location before proceeding. Proceeding without validation can result in compressing and deleting data from the wrong location.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00186",
      "content": "[com-00018] Do not assume the first directory listing matches the task's implied directory structure without explicit validation. For example, if a task states 'sub-directories for each vacation spot' (implying direct children of ~/pictures/), do not proceed if listing ~/pictures/ returns only a parent directory like 'vacations/' instead of individual vacation directories. Always compare observed structure against task expectations and document the assumption before executing irreversible operations. Failure to validate can result in operating on unintended file locations or nesting depths.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00188",
      "content": "[com-00031] Do not omit intermediate count documentation in multi-step filtering workflows. When retrieving items from paginated sources, filtering by criteria, and performing bulk operations, explicitly log counts at each stage: (1) Total items from source API(s), (2) Total items after enrichment (if applicable), (3) Total items after filtering, (4) Total items after operation completion (e.g., added to playlist). Silent failures in filtering or enrichment can occur without surfacing in final verification if intermediate counts are not tracked and documented. This documentation is essential for detecting incomplete results.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00189",
      "content": "[com-00032] Do not assume filter correctness verification requires only confirming target items match criteria. When filtering reduces a dataset significantly (e.g., 23 → 2 items), verify filter correctness by sampling excluded items and confirming they do NOT match the filter criteria. For example, when filtering for 'classical genre AND release year 2023', sample excluded songs to confirm they lack the classical genre tag or were released in different years. Verification of only target items can mask incorrect filter logic that happens to produce the right number of results by coincidence.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00195",
      "content": "[com-00033] Do not skip error recovery documentation in multi-step workflows. When an operation fails and is recovered (e.g., 401 token expiration requiring re-authentication), explicitly document: (1) The error that occurred (e.g., '401 Unauthorized'), (2) The step at which it occurred, (3) The recovery action taken (e.g., 're-authenticated with refresh token'), (4) Confirmation that the workflow resumed correctly after recovery. This documentation is essential for understanding whether the error affected data completeness and for troubleshooting similar issues in future executions.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00196",
      "content": "[com-00018] Do not filter items by date fields (e.g., release_date) before enriching items with those fields. Basic collection APIs (e.g., show_recommendations(), show_song_library()) typically do not include release_date in their responses. Always fetch enriched details for each item (e.g., via show_song()) BEFORE applying date-based filters. Filtering on missing fields will result in incorrect exclusions or silent failures.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00197",
      "content": "[com-00019] Do not extract year components from ISO 8601 date strings for comparison (e.g., extracting '2024' from '2024-01-15'). Instead, use full ISO 8601 string lexicographic comparison, as the YYYY-MM-DD format sorts correctly alphabetically without conversion. This avoids timezone handling overhead and ensures correct date ordering across all date ranges.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00200",
      "content": "[com-00018] Do not filter by enriched fields (e.g., genre, release_date, play_count) without first fetching enriched item details. Basic collection list APIs (e.g., show_song_library(), show_recommendations()) do not include enriched metadata fields. Always call the detail API (e.g., show_song()) for each item BEFORE applying filters based on enriched fields. Filtering on incomplete data will miss or incorrectly exclude items.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00201",
      "content": "[com-00019] When filtering by year or date range on ISO 8601 formatted dates, do not extract and compare the year component numerically (e.g., int(release_date[:4]) == 2024). Instead, use full lexicographic string comparison on the complete ISO 8601 date string (e.g., release_date.startswith('2024-') or release_date >= '2024-01-01' and release_date < '2025-01-01'). This avoids timezone interpretation issues and ensures correct ordering.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00202",
      "content": "[com-00020] When filtering by categorical fields that may contain compound or multi-valued data (e.g., Spotify's genre field containing 'indie-pop' or 'hip-hop-rap'), do not use exact string matching. Use substring matching or case-insensitive partial matching (e.g., 'r&b' in genre.lower()) to correctly identify items with the target category regardless of compound values or formatting.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00206",
      "content": "[com-00019] Do not assume variables created during reasoning or explanation in one step will be available in subsequent steps. Variables only persist if they are created through actual code execution (in the action field). If a step provides only reasoning without executable code, subsequent steps that reference variables from that step will fail with NameError. Always verify that each step's action field contains executable code that creates the variables needed by later steps.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00210",
      "content": "[com-00018] Do not assume 'played so far', 'so far', 'to date', or similar temporal qualifiers apply to the entire collection. These phrases explicitly limit scope to items that have been completed or are currently active, excluding future/upcoming items. When a task uses temporal language, identify the current position or state marker, then filter the collection to include only items up to and including the current position/marker. Exclude items beyond the current position. For example, 'like songs played so far' in a queue context means songs at positions 0 through current_position (inclusive), not all songs in the queue.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00214",
      "content": "[com-00024] When verifying bulk operations using aggregate or list APIs (e.g., show_liked_songs(), show_song_library()), do not assume the first page of results is complete or represents the full dataset. Always implement exhaustive pagination following [str-00025] before concluding verification passed or failed. If pagination is unavailable or returns incomplete results, fall back to item-level verification APIs (e.g., show_song_privates() for individual song state) rather than trusting the aggregate list.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00218",
      "content": "[com-00032] When filtering paginated results by a recipient or entity (e.g., 'all approved requests to Robert'), do not assume the filtered result with the maximum or minimum timestamp is correct without explicit verification. Always document: (1) total items retrieved across all pages, (2) total items matching the entity filter, (3) the timestamp of the selected extremum item, (4) whether this timestamp aligns with the task's temporal context (e.g., if the task says 'the last request I sent', verify the found request is recent, not historical). If the timestamp seems inconsistent with task context, query unfiltered or partially-filtered results to confirm no more recent items exist.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00219",
      "content": "[com-00033] When a payment or financial transaction operation is required (e.g., 'send money back', 'pay an amount'), do not execute the operation without first verifying the sender's account has sufficient balance or state to complete the transaction. Query account balance or relevant state fields before initiating the payment. After transaction creation, verify the response status field (if present) indicates success, rather than assuming None or missing status means success.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00222",
      "content": "[com-00018] When a task requires finding the 'last', 'most recent', 'oldest', or other extremum item with a specific property (e.g., 'the last payment request that was approved'), do NOT pre-filter by that property before identifying the extremum. Instead: (1) Retrieve ALL items exhaustively without pre-filters, (2) Sort by the relevant temporal or ordinal field (e.g., created_at descending for 'most recent'), (3) Identify the extremum item from the complete dataset, (4) THEN verify the extremum item satisfies the property criteria. Pre-filtering before extrema identification can result in finding the most recent item matching the filter rather than the most recent item overall that happens to match the filter.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00225",
      "content": "[com-00018] Do not skip explicit pagination exhaustion verification. After a pagination loop completes, always log the final page size and page_limit to confirm exhaustion (e.g., 'Final page had X items, page_limit was Y, pagination exhausted: X < Y'). Implicit pagination (code that happens to work) without explicit documentation can mask incomplete data retrieval in edge cases.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00226",
      "content": "[com-00019] When a task uses ordinal language like 'the last', 'the first', or 'the most recent' without explicit date context, do not assume the interpretation without verification. Always explicitly document which field (e.g., 'created_at timestamp') is used to determine the ordering and why this field represents the intended meaning (e.g., 'most recent by creation time'). Ambiguous language can lead to selecting the wrong item if multiple orderings are possible.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00227",
      "content": "[com-00020] When selecting an extremum (min/max/first/last) from a filtered subset, do not skip temporal or contextual alignment verification. After identifying the extremum candidate, explicitly verify that its timestamp or other contextual attributes align with the task's implied timeframe or context. For example, if selecting 'the last payment request to Brandon', document the request's created_at date and confirm it matches the expected recency.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00232",
      "content": "[com-00018] Do not assume bulk deletion operations completed successfully without verification. Always re-query using the identical search criteria that identified items for deletion. If the re-query returns any results, the deletion was incomplete or failed. Only after confirming zero results from the re-query can you confirm the deletion operation succeeded. This is especially critical for irreversible operations where partial deletion may leave unwanted data in place.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00236",
      "content": "[com-00020] [err-00020] Do not skip verification after bulk DELETE operations by assuming deletion succeeded without re-querying. Always re-query using the identical search/filter criteria used to identify deletion targets and confirm the result set is empty. Failure to verify can leave unwanted items in the system undetected, especially when working with paginated data where deletion may have failed for some items.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00237",
      "content": "[com-00018] Do not skip explicit checkpoint documentation when performing bulk deletion on paginated data sources. While pagination logic may be correct (checking if final page < page_limit), failing to explicitly log 'Pagination exhausted: final page had X items (< page_limit of Y)' creates an audit gap. Similarly, deletion counts ('Deleted N items total') and re-query verification results ('Re-query confirmed 0 remaining items') must be explicitly documented. Bulk deletion is irreversible; without checkpoint documentation, incomplete execution or pagination boundaries can silently cause data loss without detection.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00240",
      "content": "[com-00025] Do not conflate pagination exhaustion with search completeness. Confirming that all pages of a search result have been retrieved (pagination exhaustion) does NOT guarantee that the search parameters themselves returned all matching items. When a filtered search (e.g., search_artists(genre='classical', min_follower_count=22)) returns unexpectedly few results, implement additional validation: (1) Re-run the search without one or more filters and manually count matching items to cross-check, (2) Call detail APIs (e.g., show_artist()) on a sample of results to verify filter criteria are working correctly, or (3) Test boundary conditions (e.g., min_follower_count=21 vs. 22) to confirm filter behavior. Pagination exhaustion is necessary but not sufficient for search completeness verification.",
      "helpful": 3,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00243",
      "content": "[com-00018] Do not assume that pagination exhaustion (final page < page_limit) means a filtered search returned all matching items. Pagination exhaustion confirms all PAGES were retrieved, but does NOT confirm that search PARAMETERS are working correctly or returning complete results. Filter parameters may have behavioral quirks, interpretation issues, or API limitations that cause incomplete filtering. Always implement cross-validation for filtered searches, especially when results seem unexpectedly small or when filter parameter behavior is ambiguous.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00246",
      "content": "[com-00241] Do not assume that confirming pagination exhaustion (final page returned fewer items than page_limit) is sufficient to verify search result completeness. Pagination exhaustion only confirms that ALL PAGES of a search result were retrieved; it does NOT confirm that the SEARCH FILTER PARAMETERS themselves are working correctly or returning complete results. Filter parameters may have boundary condition quirks, interpretation issues, or API limitations that cause incomplete filtering. For example, a search with min_follower_count=21 may return incomplete results even if pagination is exhausted. Always implement two-stage verification: (1) Confirm pagination exhaustion by checking that the final page returned fewer items than page_limit, AND (2) Separately verify search completeness by cross-validating the filtered result against a baseline query (without filters or with relaxed filters) and manually counting matches to confirm the filtered result is complete.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00247",
      "content": "[com-00032] Do not assume response field structures are flat or follow a consistent pattern across different API endpoints. Some endpoints return flat structures (e.g., 'receiver_email' as a direct field) while others return nested objects (e.g., 'receiver' as an object with 'email' as a sub-field). When accessing response fields in verification or any code, always inspect the API specification via show_api_doc() first to confirm the exact field path. Accessing a non-existent field path returns None silently without raising an exception, which can mask verification failures.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00251",
      "content": "[com-00018] When performing financial transactions (e.g., Venmo requests) in multi-app workflows, do not skip specification inspection before making API calls. Always inspect the target API's documentation (e.g., Venmo request schema) to identify all required and optional parameters, including description/note fields. Failing to check specifications upfront can result in requests with missing or incorrectly formatted parameters, requiring rework.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00252",
      "content": "[com-00019] When creating financial transaction requests (e.g., Venmo payment requests) with user-specified descriptions or notes, do not paraphrase or interpret the description. Use the exact description provided in the task specification. For example, if the task specifies 'internet bill for the last month', do not use variations like 'monthly internet' or 'internet payment'. Exact parameter matching is critical for audit trails and user expectations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00259",
      "content": "[com-00018] When a task involves cost-sharing or bill-splitting with a statement like 'shared equally among my roommates and me', do not divide only by the number of roommates. The payer is included in the group. Always divide the total by (number of roommates + 1). Dividing only by roommates results in incorrect per-person amounts and over-charges to roommates.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00262",
      "content": "[com-00017] Do not assume authentication credentials are automatically managed across API calls within a workflow. Each API call that requires authentication must explicitly receive the credential parameter (e.g., access_token). Obtain credentials from supervisor.show_account_passwords() and store them in variables, then pass them to each subsequent call. Failing to pass credentials results in 401 Unauthorized errors.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00263",
      "content": "[com-00018] Do not assume non-existent APIs exist on a service (e.g., supervisor.get_credentials()). Always consult show_api_doc() before making an API call to verify the API exists and understand its exact parameters and return structure. Attempting to call non-existent APIs results in errors that require recovery via documentation lookup.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00264",
      "content": "[com-00019] Do not reference or use variables before they are created in the execution context. When starting a new workflow or step, explicitly create required variables (e.g., authenticate to obtain access_token) before attempting to use them in subsequent API calls. Attempting to reference undefined variables results in NameError.",
      "helpful": 1,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00266",
      "content": "[com-00018] [err-00018] Do not assume parameter names, types, or structures for API calls without first calling show_api_doc() to verify the specification. Even if a call succeeds without this check, it represents a gap in defensive programming that could cause failures in edge cases, with different API versions, or when parameters are optional vs. required. This is especially critical for unfamiliar endpoints, service transitions, and irreversible operations like task completion or account changes.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00268",
      "content": "[com-00018] Do not assume search_notes() or similar search APIs return all matching results on the first page. Search APIs often implement pagination with limited default page sizes (e.g., 5 results per page). When searching for a specific item by name or criteria, implement exhaustive pagination (continue fetching with incrementing page_index until response size < page_limit) to ensure the target item is not missed on subsequent pages.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00269",
      "content": "[com-00019] Do not attempt to call data retrieval or modification APIs (e.g., search_notes(), update_note()) without first obtaining authentication credentials and logging in. Unauthenticated API calls will return 401 errors. Always obtain credentials from the supervisor and authenticate as a prerequisite step before any data operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00274",
      "content": "[com-00208] Do not assume that re-authenticating in each step is necessary or efficient. If an access token or session variable is created in an earlier step, reuse it in subsequent steps rather than calling login/authentication APIs repeatedly. Repeated authentication wastes API calls and increases latency. Store tokens and session objects as variables and pass them to downstream steps through the action field's executable code.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00280",
      "content": "[com-00302] Do not assume API response structures without verification. Before accessing fields on an API response (e.g., assuming show_alarms() returns a dict with an 'alarms' key), verify the actual response structure by inspecting show_api_doc() output and testing with isinstance() checks. Assumptions about response shape (dict vs list, presence of specific keys) lead to AttributeError or KeyError exceptions.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00281",
      "content": "[com-00303] Do not assume credential formats or authentication parameter names. When extracting credentials from show_account_passwords(), verify it returns a list (not a dict), iterate through entries to match 'account_name', and verify the login API's required username field (e.g., phone number vs email). Incorrect credential handling prevents authentication and blocks all subsequent operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00285",
      "content": "[com-00304] Do not assume authentication credentials are in dict format or use email as a username when the API specification requires phone number. Always retrieve show_account_passwords(), inspect its actual return type (typically a list of dicts), extract credentials by iterating and matching 'account_name', and verify the login API's username parameter requirement. Store the returned access token in a variable for reuse across subsequent authenticated API calls.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00286",
      "content": "[com-00018] Do not skip the aggregation step when finding extrema across items with sub-components. When the extrema metric is derived from multiple sub-items (e.g., total duration of a playlist is the sum of all song durations), always: (1) Fetch enriched details for each sub-item to obtain the metric value, (2) Aggregate the metric across all sub-items for the parent item (e.g., sum durations for all songs in a playlist), (3) THEN apply the comparison function to identify the extremum across parent items. Skipping aggregation or comparing individual sub-item metrics instead of aggregated parent metrics will produce incorrect results.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00287",
      "content": "[com-00019] When a task requires rounding or specific numeric formatting (e.g., 'rounded to the nearest number'), do not assume the metric field from the API is already in the required format. Always apply the specified formatting function (e.g., round(), floor(), ceil()) to the final result before returning it. For duration metrics in minutes, verify whether the API returns seconds or fractional minutes, and convert/round accordingly to match the task requirement.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00289",
      "content": "Do not skip aggregation steps when finding extrema across nested collections (e.g., finding the shortest playlist). When a collection contains sub-items with metrics (e.g., playlists contain songs with durations), you must: (1) Fetch enriched details for all sub-items, (2) Aggregate the metric at the collection level (e.g., sum song durations per playlist), (3) THEN apply the comparison function (min/max) to the aggregated values, not to individual sub-item metrics. Skipping aggregation results in comparing the wrong metric (e.g., comparing individual song durations instead of total playlist durations).",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00291",
      "content": "[com-00205] Do not provide pseudocode, explanations, or reasoning in the action field instead of actual executable code. The action field must contain runnable Python code that creates persistent variables. Reasoning and pseudocode belong in the thought field or as code comments. Code written only in the thought field does not create variables available to subsequent steps.",
      "helpful": 0,
      "harmful": 4,
      "section": "common_mistakes"
    },
    {
      "id": "com-00292",
      "content": "[com-00206] Do not leave action fields empty or write only pseudocode when a step has sequential dependencies. If a subsequent step depends on variables created in the current step, the current step's action field must contain actual executable code. Empty action fields cause implicit NameError failures in dependent steps.",
      "helpful": 1,
      "harmful": 1,
      "section": "common_mistakes"
    },
    {
      "id": "com-00293",
      "content": "[com-00207] When finding extrema across nested collections with aggregated metrics (e.g., 'longest playlist'), do not apply the extrema function directly to sub-item metrics (e.g., individual song durations). Instead: (1) Aggregate the metric at the parent level (sum song durations per playlist), (2) Apply max/min to the aggregated parent-level values (not sub-item values), (3) Format the result as specified (e.g., round to nearest integer). Applying extrema to sub-item metrics yields incorrect results.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00298",
      "content": "[com-00018] Do not assume the structure or type of an API response without first consulting the API specification via show_api_doc(). For example, do not assume login() returns a string token; always verify whether it returns a dictionary with nested fields (e.g., {'access_token': '...', 'token_type': '...'}) before attempting to slice or access the response. Failing to inspect the spec first can result in TypeErrors or incorrect field extraction.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00299",
      "content": "[com-00019] When finding extrema (min/max) across enriched item lists, do not manually iterate and compare values. Instead, use Python's built-in min() or max() function with a key parameter (e.g., min(songs, key=lambda s: s['play_count'])) to identify the extremum item directly. This is more efficient, less error-prone, and returns the full item object for subsequent operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00302",
      "content": "[com-00018] Do not skip verification of action API responses before marking task completion. When executing a primary action (e.g., play_music(), send_payment(), create_resource()), always inspect the response object for success indicators (e.g., 'message', 'status', 'success' fields) before calling apis.supervisor.complete_task(). A successful HTTP response does not guarantee the action persisted or executed as intended. Verify the response contains expected fields and success messaging (e.g., 'now playing', 'payment sent', 'created') before proceeding to completion marking.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00303",
      "content": "[com-00019] Do not assume you understand an action API's response schema without inspecting its documentation first. Before calling action APIs (e.g., play_music(), rate_song(), update_profile()), always call show_api_doc('[api_name]') to retrieve the response structure, success indicators, and error field names. This prevents misinterpretation of response fields and ensures you check the correct fields for success/failure status.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00308",
      "content": "[com-00082] When search APIs return multiple results with similar or overlapping names (e.g., 'Velvet Underground' matching both 'The Velvet Underground' and 'Aria Sterling - Velvet Underground'), do not assume the first result is the correct match without verification. Always inspect all search results, examine disambiguating metadata (artist name, release date, genre, popularity), and either re-search with a more specific query or explicitly verify the selected result matches the task's implied intent before proceeding with dependent operations (e.g., extracting songs from the album).",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00311",
      "content": "[com-00302] Do not stop execution after completing the primary task action (e.g., accepting payment requests, rating songs) without executing the final completion marking step. Tasks are not considered complete in the system until apis.supervisor.complete_task(answer=summary) is called with a descriptive answer. Omitting this step leaves the task in an incomplete state regardless of whether the primary action succeeded. The completion step is mandatory and must be executed as the final action.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00315",
      "content": "[com-00018] Do not assume API response structure based on common patterns (e.g., assuming a dict with a 'results' or 'data' key). Always call apis.api_docs.show_api_doc() to explicitly verify the response schema before writing code that accesses response fields. For example, some APIs return a list directly, while others wrap results in a dict. Assuming the wrong structure causes AttributeError or KeyError. When consolidating code from multiple steps, inspect the actual response type from prior successful executions rather than reimplementing from memory.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00319",
      "content": "[com-00018] Do not ignore task qualifiers or scope-limiting phrases (e.g., 'from my coworkers and friends', 'involving any of [list]', 'to specific people'). These are filtering requirements, not just context. Treating a qualifier-constrained task as unconstrained (e.g., 'accept all pending requests' instead of 'accept pending requests FROM coworkers and friends') causes operations to be applied to the wrong set of items. Always identify the qualifier, resolve the target entities upfront, and filter before executing bulk operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00323",
      "content": "[com-00018] Do not assume search APIs (e.g., search_songs()) return only items matching a single narrow criterion. Search APIs typically return all items where the query term appears in any searchable field (title, description, artist name, etc.), resulting in broad result sets. When searching for items by a specific attribute (e.g., songs by artist 'Velvet Echo'), implement filtering logic to isolate items where that attribute matches exactly (e.g., artist_id is in the artists array) AFTER pagination. Consider whether a more targeted API endpoint (e.g., show_artist() to get artist's songs directly) is available before relying on search with post-filtering.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00327",
      "content": "[com-00162] Do not assume that final or irreversible API operations (task completion, account termination, data deletion) succeeded without verifying the response. Even if an API call executes without throwing an exception, the operation may have failed silently or the parameter may not have been accepted. Always inspect the response object for error fields, status indicators, or confirmation messages before considering the operation complete. For example, a task completion call may execute but return a response where final_answer is null, indicating the answer parameter was not properly registered.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00330",
      "content": "[com-00018] When finding superlatives (most/least played, most/least liked, etc.) across any collection, do not assume the first API response contains all relevant items. Even if the response contains results, there may be additional pages. Always check the API specification for pagination parameters (page_index, page_limit, next_page, etc.) before making any calls. If pagination parameters exist, exhaustive pagination is mandatory—continue fetching with incrementing page_index or next_page tokens until the response size is less than page_limit or empty. Stopping after the first page produces incomplete datasets and incorrect superlative answers.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00333",
      "content": "[com-00018] Do not assume field names in API response objects without consulting the API specification first. Field naming conventions vary across APIs (e.g., 'id' vs 'artist_id' vs 'artist.id'). When a field name assumption is wrong, the API returns None silently instead of raising an exception, causing downstream logic to fail without clear error signals. Always call show_api_doc() or reference the API schema before accessing response fields, even if you believe you know the structure.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00334",
      "content": "[com-00019] Do not perform incomplete pagination during verification steps. Checking only the first page of a paginated response can produce false negative results (e.g., 'all items were followed' when only the first page was checked). Verification steps must implement exhaustive pagination with the same rigor as initial data collection, continuing until response size < page_limit or empty, to ensure the complete dataset is compared.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00338",
      "content": "[com-00018] When performing bulk operations based on set-difference filtering (e.g., 'unfollow artists with no liked songs'), do not skip explicit pagination exhaustion documentation at each data aggregation stage. Even if pagination is correctly implemented, failing to explicitly document that each source was exhaustively paginated (e.g., 'Retrieved 9 followed artists across X pages', 'Retrieved 38 liked songs across Y pages') can mask incomplete data collection if pagination logic contains subtle bugs. Always log pagination completion metrics before proceeding to set operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00339",
      "content": "[com-00018] Do not assume field names or response structures in API responses without consulting the API specification first. Always call apis.api_docs.show_api_doc(app_name='...', api_name='...') BEFORE writing field access code to inspect exact field names, types, and response schema. This is especially critical in verification steps where incorrect field access (e.g., accessing 'id' instead of 'artist_id') silently returns None without raising exceptions, masking incomplete operations and producing false-negative verification results. The pattern should be: (1) Call show_api_doc() to inspect specification, (2) Document exact field names and structure, (3) Write field access code using verified names, (4) Test on sample responses before full verification.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00342",
      "content": "[com-00020] Do not omit task completion marking after verification succeeds. After verifying that bulk operations persisted via re-querying, explicitly mark the task as complete in the system using the appropriate completion API. Omitting this step leaves the task in an open state even though all work is done, creating ambiguity about whether the task was actually finished.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00345",
      "content": "[com-00019] Do not assume that writing correct Python code in the 'thought' field satisfies a task requirement. Even if the code is syntactically correct and logically sound, it has no effect unless executed in the 'action' field. The 'observation' field will show no execution results, no side effects will occur (files won't be created, APIs won't be called, accounts won't be terminated), and the task will remain incomplete. Always move executable code from 'thought' to 'action' before considering a step complete.",
      "helpful": 0,
      "harmful": 1,
      "section": "common_mistakes"
    },
    {
      "id": "com-00349",
      "content": "[com-00XXX] Do not write executable code in the 'thought' field expecting it to persist or execute. The 'thought' field is for reasoning and planning only; variables and API calls defined there do not carry forward to subsequent steps. All executable Python code must be placed in the 'action' field. If the 'action' field is empty, the observation will show 'No code to execute' and no API calls will be made, leaving the task incomplete. Always verify that code is in the 'action' field, not just in the 'thought' field.",
      "helpful": 0,
      "harmful": 1,
      "section": "common_mistakes"
    },
    {
      "id": "com-00352",
      "content": "[com-00018] Do not assume the parameter names, types, or response structure of final/irreversible API calls (e.g., complete_task(), delete_account(), terminate_service()). Always call show_api_doc() to inspect the endpoint specification BEFORE constructing the request. Assuming parameter names (e.g., 'answer' vs. 'response' vs. 'result') or response fields (e.g., 'final_answer' vs. 'answer' vs. no return field) can result in silent failures where the operation appears to succeed (e.g., 'task marked complete') but critical data is not properly registered in the system.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00356",
      "content": "[com-00026] Do not abandon multi-step task workflows in the planning phase without executing any code. If the 'action' field is empty and the 'observation' returns 'No code to execute. Continue reasoning.', the workflow has stalled and no progress has been made toward the task goal. Consolidate all executable code into the 'action' field immediately to begin execution.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00357",
      "content": "[com-00027] Do not complete a task without calling apis.supervisor.complete_task(answer=...) to mark it complete in the system. Tasks are not considered complete until this API call is made. Omitting this step leaves the task in an incomplete state even if the answer has been calculated.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00365",
      "content": "[com-00018] Do not assume parameter names based on task context or analogous APIs. When calling an unfamiliar API (especially external services), parameter names may differ from conventional naming (e.g., 'username' vs. 'user_id' vs. 'email'). Always call show_api_doc() to verify exact parameter names before the first API call. A 422 validation error indicates incorrect parameter names or structure, not logic errors—re-inspect the API spec rather than attempting alternate logic.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00366",
      "content": "[com-00019] Do not place executable code in the thought field. Code written in thought fields is not executed, and variables created there are not available to subsequent steps or action fields. All Python code (API calls, variable assignments, data transformations) must be in action fields. Pseudocode or planning logic may be in thought fields, but actual executable statements must be in action fields to ensure they run and maintain variable scope across the workflow.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00367",
      "content": "[com-00020] Do not proceed to the next step without verifying that the current step's API call succeeded. Always check the response for error indicators (e.g., 'error' field, HTTP status codes, or empty/null values) before using response data in subsequent steps. If an API call fails (e.g., login returns an error), do not attempt to use the expected response fields (e.g., access_token) in the next step, as they will not exist and will cause downstream failures.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00372",
      "content": "[com-00017] When filtering payment requests or similar multi-participant transaction objects by contact criteria, do not filter on only one participant field (e.g., only 'sender'). Payment requests have directional semantics: incoming requests have the requester as 'sender' and the current user as 'receiver'. Verify the correct participant field for the task direction (e.g., 'requests FROM my contacts' filters on 'sender' field) before applying OR logic filtering.",
      "helpful": 0,
      "harmful": 0,
      "section": "common_mistakes"
    },
    {
      "id": "com-00373",
      "content": "[com-00018] Do not use nested f-strings or complex list comprehensions with inner f-strings in the final completion API call. String formatting syntax errors in the completion step are catastrophic because they prevent marking the task complete in the system, even though all prior work (e.g., approvals, updates) succeeded. The completion call is the irreversible gate between 'work done' and 'task closed'; any failure here leaves the task incomplete. Always use simple, tested string formatting (e.g., separate list comprehension from f-string, avoid nested f-strings) before the completion call.",
      "helpful": 0,
      "harmful": 0
    }
  ],
  "strategies_and_hard_rules": [
    {
      "id": "str-00009",
      "content": "[strat-00009] For bulk operations on items with pre-existing state (e.g., ratings, reviews, flags): (1) Query the current state of all target items using appropriate filters, (2) Categorize items by their current state (missing state vs. existing state with specific values), (3) Apply create operations for items missing state, (4) Apply update operations for items with existing state that needs modification, (5) Verify final state matches expectations by re-querying a sample or all items.",
      "helpful": 3,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00014",
      "content": "[strat-00010] When a task statement contains multiple clauses or conditional logic, decompose it into independent requirements and constraints: (1) Identify the primary action(s) (e.g., 'rate non-liked songs at 1-star'), (2) Identify any conditional rules (e.g., 'if rated higher, decrease to 1'), (3) Determine the scope of each rule by analyzing whether it logically applies only to the primary action's subset or to the entire collection, (4) Apply each rule to its correct scope. Treat constraint-based rules (e.g., 'no value exceeds X') as universal filters unless explicitly scoped to a subset.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00018",
      "content": "[strat-00011] For bulk state operations with conditional rules on filtered subsets: (1) Explicitly decompose the task to identify primary action scope and conditional rule scope, (2) Confirm both scopes are identical by analyzing the task language (e.g., 'which I have liked' restricts BOTH the primary action AND the conditional rule), (3) Document the scope boundary explicitly (e.g., 'This applies only to liked songs; non-liked songs are out of scope'), (4) Query state only for items within the confirmed scope, (5) Verify both in-scope items AND a sample of out-of-scope items to ensure no unintended modifications.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00025",
      "content": "[strat-00012] For bulk operations on paginated data sources with universal quantifier scope (e.g., 'all transactions', 'every contact'): (1) Implement a pagination loop that exhaustively retrieves all pages before applying any filters or operations, (2) Continue fetching with incrementing page_index until the response size is less than page_limit or empty, (3) Accumulate all items from all pages into a single collection, (4) Apply filtering, deduplication, and operations to the complete collection, not to individual pages. This ensures no relevant items are missed due to pagination boundaries.",
      "helpful": 63,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00032",
      "content": "[strat-00013] For tasks that aggregate data from multiple paginated sources and then perform irreversible operations (e.g., account termination): (1) Complete all data aggregation and verification steps BEFORE initiating the irreversible operation, (2) Verify pagination exhaustion for each source independently, (3) Document deduplication counts and final output integrity, (4) Only after all verification passes, proceed to the irreversible operation. This prevents loss of data or account access due to incomplete backups.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00038",
      "content": "[strat-00014] For data aggregation from multiple paginated sources followed by irreversible operations: (1) Implement exhaustive pagination for each source independently, explicitly documenting final page size vs. page_limit for each, (2) Deduplicate across all sources and document before/after counts, (3) Create output file with specified format, (4) Read file back to verify format compliance (headers, delimiters, field counts, total lines), (5) Create an explicit verification checkpoint that logs all intermediate counts and format checks, (6) Only after checkpoint passes, execute the irreversible operation. This ensures data integrity and prevents loss of access to backup data.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00040",
      "content": "[strat-00014] For multi-app workflows requiring data retrieval and communication: (1) Identify all required services and authenticate to each independently before making API calls, (2) Use search/query APIs to locate target entities (contacts, notes, records) before attempting detail retrieval, (3) Understand the structure of returned data (e.g., note content as formatted text) and extract relevant information programmatically (e.g., parsing movie titles from structured note content), (4) Verify successful completion of each step before proceeding to dependent steps, (5) Format extracted data according to task specifications before executing final communication or irreversible actions, (6) Only execute the final action (e.g., send message) after all data gathering, extraction, and formatting steps are complete and verified.",
      "helpful": 18,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00049",
      "content": "[strat-00014] When calling an API endpoint for the first time or when uncertain about parameter names: (1) Before attempting the API call, retrieve the endpoint specification using apis.api_docs.show_api_doc(service_name, endpoint_name), (2) Document the exact parameter names, types, required vs optional status, and expected values, (3) Cross-reference parameter names against similar endpoints in other services to identify naming inconsistencies, (4) Only then construct and execute the API call with verified parameters. This 'specification-first approach' prevents parameter validation errors and reduces debugging cycles, especially when working across multiple services with different naming conventions.",
      "helpful": 55,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00052",
      "content": "[strat-00014] For bulk operations that query multiple entities with OR logic (e.g., 'payments from ANY of [list of coworkers]'): (1) Extract all target entity identifiers upfront (e.g., coworker emails via apis.phone.search_contacts()), (2) Implement a loop that queries each entity independently with exhaustive pagination, (3) Accumulate all results across all entities into a single collection, (4) Apply deduplication if the same item could appear in multiple entity queries, (5) Apply subsequent filters or operations to the complete aggregated collection. This pattern ensures no items are missed due to pagination boundaries or entity-level filtering.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00078",
      "content": "[strat-00014] For multi-app workflows requiring data extraction and cross-service operations: (1) Authenticate to each service independently using provided credentials before making any API calls, (2) Complete all data gathering, parsing, and reconciliation BEFORE initiating irreversible operations (e.g., payment requests), (3) Parse unstructured content (e.g., notes) into structured data using appropriate delimiters or parsing logic, (4) Cross-reference extracted data against existing records in the primary service (e.g., compare parsed payment obligations against transaction history to identify deltas), (5) Search/query secondary systems to locate target entities (e.g., find Venmo user IDs) before bulk operations, (6) Execute operations on the identified subset, (7) Verify completion by re-querying the operation results. This pattern ensures data consistency across services and prevents duplicate or incorrect operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00085",
      "content": "[strat-00014] For multi-step data reconciliation tasks across unstructured sources and external APIs: (1) Extract and parse unstructured data completely, (2) Immediately verify extraction completeness (count, sum, no duplicates), (3) Match extracted entities against API results with explicit confidence documentation, (4) Create reconciliation checkpoints before irreversible operations: verify total source items = processed items + excluded items, verify sums match expected values, verify no items processed twice, (5) Only after all reconciliation passes, proceed to irreversible operations (e.g., creating requests, payments). This prevents cascading failures from incomplete data extraction or mismatched entities.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00088",
      "content": "[str-00033] When verifying bulk operations on paginated data sources, apply the same exhaustive pagination strategy to the verification query as to the initial data retrieval. Do not assume the first page of verification results is complete. Implement pagination loops in verification steps to ensure all items are re-queried and confirmed, not just a sample from the first page. This is especially critical when verifying that irreversible operations (e.g., payment requests) were created correctly across all target entities.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00091",
      "content": "[strat-00014] For multi-entity filtering tasks with aggregation: Always resolve target entities by relationship or other criteria BEFORE initiating pagination on the data source. This prevents unnecessary pagination of the entire dataset and ensures the filter criteria are well-defined and complete before filtering begins. Document the count of identified target entities as part of the verification baseline.",
      "helpful": 3,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00098",
      "content": "[str-00026] For tasks with relative temporal references (e.g., 'since 1st Mar of this year', 'transactions from this month'): (1) Call the appropriate date/time API (e.g., apis.phone.get_current_date_and_time()) to retrieve the system's current date, (2) Parse the returned date string to extract the relevant time component (year, month, etc.), (3) Construct the date filter parameter (e.g., min_created_at) using the extracted component, not hardcoded values, (4) Document the current date and the derived filter value explicitly (e.g., 'Current date: 2024-12-15, using min_created_at: 2024-03-01'), (5) Include this documentation in API calls and verification steps. This pattern prevents temporal off-by-one errors and ensures filters align with task intent.",
      "helpful": 3,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00101",
      "content": "[strat-00014] For multi-step workflows that retrieve the same entity from multiple API endpoints and perform operations based on that entity: (1) After retrieving entities from the first endpoint, explicitly document the response schema by inspecting sample responses and recording exact field names and types, (2) Before proceeding to subsequent operations (e.g., following artists), verify that extracted entity identifiers and metadata match the expected schema, (3) After retrieving entities from subsequent endpoints (e.g., show_following_artists()), explicitly cross-validate that the same entities are represented consistently across both endpoints by comparing both ID and name/metadata fields, (4) Document any discrepancies or schema differences and resolve them before considering the workflow complete. This prevents silent failures where operations succeed mechanically but entities are mismatched or inconsistent across sources.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00110",
      "content": "[strat-00014] For bulk filtering operations on categorical fields (e.g., 'follow all artists of indie-genre songs'): (1) Inspect the API schema via show_api_doc() to understand the field's format and possible value variations, (2) Implement filtering logic appropriate to the field structure (substring matching for compound values, exact equality only if schema guarantees single-word exact values), (3) Test the filter logic on a sample of 3-5 items to confirm correct identification before applying to the full dataset, (4) Document the filtering logic and the rationale for the chosen approach (exact vs. substring matching), (5) Proceed with subsequent operations (extraction, deduplication, bulk actions) on the filtered dataset.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00113",
      "content": "[strat-00014] For tasks with time-bounded universal quantifier scope (e.g., 'total cost for this year,' 'all bills in 2023'): (1) Interpret the time scope as the full calendar period (Jan-Dec for 'this year') unless context explicitly limits it to a partial range, (2) Do NOT assume current date (e.g., May 18) limits the scope to Jan-May; it only indicates which bills are likely available, (3) Verify scope by checking for data across the entire calendar period before concluding data collection, (4) Document expected item count (12 months for a year) and actual items found, (5) Flag missing periods explicitly before providing final aggregation. This ensures time-bounded aggregations include all expected periods, not just available-to-date data.",
      "helpful": 5,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00124",
      "content": "[str-00026] For multi-format file aggregation tasks (e.g., extracting data from both .txt and .pdf files): (1) List all files in the target directory recursively, (2) Group files by data period (e.g., month) and format, (3) For each period, implement priority-based extraction: attempt primary format first (e.g., .txt), (4) If primary format extraction fails or is unavailable for a period, immediately attempt secondary formats (e.g., .pdf) without waiting for all primary formats to complete, (5) For each file, if initial parsing fails, attempt decoding (base64, hex) and decompression (gzip, zlib) before concluding the file is inaccessible, (6) Accumulate results by period, documenting which formats were successfully extracted for each period, (7) Proceed to verification only after exhausting all format and decoding attempts for all periods.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00143",
      "content": "[strat-00015] For metadata-driven file reorganization tasks: (1) Authenticate to file_system independently, (2) Fetch enriched metadata for all files in the source directory using exhaustive pagination, (3) Parse categorical components from metadata (e.g., month-year from timestamps), (4) Group files by category, (5) Create all destination containers upfront, (6) Move files with retain_dates=True to preserve original metadata, (7) Verify reorganization by confirming source is empty and all files are in correct destinations with original names intact, (8) Document counts at each stage: initial file count, per-category counts, final verification count.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00146",
      "content": "[strat-00014] For bulk removal operations across multiple independent collections (e.g., song library + multiple playlists): (1) Identify all target collections upfront, (2) Retrieve items from all collections with exhaustive pagination, (3) Deduplicate items by unique identifier across collections, (4) Enrich deduplicated items with required filter fields (e.g., release_date), (5) Apply filtering logic to identify items for removal, (6) For each target collection, remove filtered items independently using the collection-specific removal API, (7) Verify removal from each collection separately—do not assume removal from one collection affects others. This pattern ensures complete removal across all target collections without unintended side effects.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00154",
      "content": "[str-00026] [strat-00014] For multi-step task workflows with sequential dependencies (retrieve → create/find → act → verify → complete): (1) Identify all steps required to move from task initiation to task completion, (2) Execute steps in order, ensuring each step's output feeds into the next, (3) After the primary action step (e.g., play_music, send_message), explicitly execute the verification step to confirm success from the API response, (4) Only after verification passes, execute the completion marking step (e.g., apis.supervisor.complete_task()) with a summary of actions taken, (5) Document that the completion marking is the terminal step that closes the task in the system. Do not stop execution after the primary action; the task is incomplete without the completion marking.",
      "helpful": 35,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00156",
      "content": "[str-00154] For multi-step task workflows with sequential dependencies (e.g., gather input → search resources → create/modify → execute action → verify → mark complete): (1) Decompose the task into ALL required phases: data gathering, resource search, resource creation/modification, data preparation (aggregation/verification), primary action execution, verification of action success, and task completion marking, (2) Execute phases strictly in order—do not skip or combine phases, (3) Before proceeding to the primary action phase (e.g., play_music), complete ALL preceding phases including data preparation and verification, (4) After executing the primary action, verify success (check response status, confirm resource state change), (5) ALWAYS execute the final completion marking step (supervisor.complete_task) with a summary of what was accomplished—task is NOT complete until this step executes, (6) Document intermediate outputs (e.g., playlist ID, song count, total duration) for inclusion in the completion summary. Stopping at any intermediate phase leaves the task incomplete in the system.",
      "helpful": 11,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00163",
      "content": "[str-00049] [strat-00014] Always follow a 'specification-first approach' for API calls: (1) Before calling ANY endpoint, especially unfamiliar ones or those that are final/irreversible (task completion, deletions, account changes), retrieve the full API specification using `apis.api_docs.show_api_doc(app_name='[app]', api_name='[endpoint]')`, (2) Inspect the response to document: parameter names, parameter types (string, integer, list, object, etc.), required vs. optional fields, valid enum values, and return type, (3) Construct the API call to match the specification exactly, formatting parameters to the correct type (e.g., convert dict to string if API expects string), (4) Only after specification verification is complete, proceed with the API call. This prevents parameter validation errors, type mismatches, and reduces debugging cycles, especially on irreversible operations where a single error can fail the entire workflow.",
      "helpful": 46,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00177",
      "content": "[strat-00014] When performing multi-step operations on file system or similar resources (e.g., compress and delete directories): (1) Explore the target API's parameter documentation to identify compound parameters that can accomplish multiple related operations atomically, (2) Prefer atomic compound operations over sequential separate API calls when available (e.g., compress_directory with delete_directory=True instead of calling compress_directory followed by delete_directory separately), (3) Document which operations are performed by the single atomic call, (4) Verify the final state reflects completion of ALL operations intended by the compound parameter. Atomic operations reduce API overhead and eliminate race conditions or partial completion scenarios.",
      "helpful": 2,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00181",
      "content": "[str-00033] For file system operations involving irreversible actions (compress, delete, move): (1) Parse the task description to extract the expected source and destination paths explicitly, (2) List the source directory with recursive=false to inspect DIRECT children and confirm they match the task expectation, (3) If the observed structure does not match the task description (e.g., expected direct children but found nested structure), stop and investigate before proceeding, (4) Document the source and destination paths explicitly before executing the irreversible operation, (5) Execute the operation only after confirming the paths are correct. This prevents data loss from operating on unintended file locations.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00185",
      "content": "[str-00181] For file system operations involving irreversible actions (compress with delete, bulk delete, move with overwrite): (1) Parse the task description to extract the expected source directory structure and nesting depth (e.g., 'sub-directories of ~/pictures/' implies direct children), (2) List the source directory with recursive=false and appropriate filters to inspect DIRECT children only, (3) Explicitly compare the observed directory structure against the task specification, documenting any discrepancies (e.g., 'Expected vacation directories as direct children of ~/pictures/ but found only vacations/ directory with nested contents'), (4) If the observed structure does not match expectations, STOP and investigate before proceeding—do not assume the first listing is complete or correct, (5) Only after confirming the structure matches task intent, proceed with the irreversible operation. This prevents data loss from operating on unintended file locations or nesting depths.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00190",
      "content": "[str-00033] For multi-step workflows involving data retrieval, filtering, and bulk operations (e.g., retrieve recommendations → filter by criteria → add to playlist): (1) Establish a formal pagination checkpoint after exhaustive retrieval, documenting page counts and confirming pagination exhaustion, (2) Document intermediate counts after enrichment (if applicable), (3) Document counts after filtering with explicit filter criteria applied, (4) Perform comprehensive verification covering both target items (confirm match criteria) and excluded items (confirm do NOT match criteria), (5) Establish a final verification checkpoint before completing the task, confirming all verification checks have passed. This ensures the complete filtering pipeline is transparent and verifiable.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00198",
      "content": "[strat-00014] For tasks with temporal scope requirements (e.g., 'songs released this or last year'): (1) Retrieve the current date explicitly as the first step using an appropriate API or system call, (2) Use the current date to calculate the temporal boundaries (e.g., start of last year, end of current year), (3) Document the calculated boundaries explicitly for verification, (4) Use these boundaries in all subsequent filtering operations. Do not assume temporal scope or rely on implicit date handling.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00207",
      "content": "[str-00033] For multi-step workflows with sequential dependencies (e.g., retrieve → process → verify → complete): (1) Identify all required phases and their dependencies upfront, (2) In each step's action field, write actual executable code (not pseudocode or explanation) that completes the phase and creates output variables, (3) Ensure variables created in one step are captured and passed to the next step (e.g., store song_ids in a list, return it, use it in the next step's code), (4) Execute steps strictly in dependency order with no skipping, (5) Before proceeding to the next step, verify the current step's code executed without NameError or other exceptions. Reasoning and planning should occur before the action field is written, not within it.",
      "helpful": 9,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00215",
      "content": "[strat-00014] When planning verification steps for bulk operations: (1) Identify the verification method (aggregate API vs. item-level API), (2) Call apis.api_docs.show_api_doc() to inspect whether the verification API supports pagination parameters, (3) If pagination is supported, implement exhaustive pagination before verifying results, (4) If the aggregate API is paginated and may return incomplete results, prefer item-level verification APIs (e.g., show_song_privates() instead of show_liked_songs()) to directly query individual item state, (5) Document which verification method was chosen and why. This ensures verification is reliable and not misled by incomplete paginated responses.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00221",
      "content": "[str-00026] For tasks with relative temporal references (e.g., 'the last request', 'the most recent payment', 'an accident that just happened'): (1) After filtering and identifying the extremum result, explicitly compare the result's timestamp against the task's implied recency. (2) If the found item's timestamp is significantly older than the task context suggests (e.g., May 2023 when task implies 'recent'), query alternative filters (e.g., all requests regardless of status, or requests in a recent date range) to confirm no more recent matching items exist. (3) Document the temporal alignment explicitly before proceeding with operations: 'Task implies [timeframe]. Found item dated [date]. Status: [aligned/misaligned/requires clarification].' (4) If misalignment is detected, flag for clarification rather than proceeding with potentially incorrect items.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00231",
      "content": "[str-00033] For multi-stage filtering and extrema selection workflows: (1) Implement exhaustive pagination on the source API (document final page size vs. page_limit), (2) Apply all in-code filters to the complete dataset (document counts before/after each filter), (3) Clarify ambiguous task language (e.g., what 'last' means) before selecting the extremum, (4) Select the extremum using an explicitly documented field (e.g., sort by created_at descending), (5) Verify temporal or contextual alignment of the selected item, (6) Log all intermediate counts and decisions for auditability. This ensures no data is lost and the final selection is justified and verifiable.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00242",
      "content": "[str-00033] For bulk operations that depend on search or filter results (e.g., 'follow all classical artists with 22+ followers'): (1) Execute the filtered search with exhaustive pagination, (2) Verify pagination exhaustion per [str-00025], (3) SEPARATELY verify search completeness by implementing one or more cross-validation techniques: (a) re-run the search without filters and manually count matches, (b) test boundary conditions on numeric filters, or (c) sample the result set and verify filter criteria via detail APIs, (4) Only after both pagination exhaustion AND search completeness are confirmed, proceed with bulk operations. This two-stage verification prevents incomplete bulk operations caused by undetected search parameter issues.",
      "helpful": 3,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00244",
      "content": "[str-00033] For bulk operations depending on filtered search results (e.g., 'follow all EDM artists with 23+ followers'): (1) Execute the filtered search with exhaustive pagination and confirm pagination exhaustion (final page < page_limit), (2) SEPARATELY verify search completeness by cross-validation: (a) Re-run the search without the filter or with a relaxed filter value (e.g., min_follower_count=1 instead of 23) to establish a baseline of total matching items, (b) Manually verify that results from the unfiltered search meet the original filter criteria (e.g., spot-check follower counts), (c) Compare filtered result count against the count of items from the unfiltered search that meet the filter criteria, (d) If counts diverge significantly, investigate filter behavior before proceeding, (3) Alternatively, test boundary conditions (e.g., search with filter value one below threshold) to confirm filter transitions work as expected, (4) Document both pagination exhaustion verification AND search completeness verification before proceeding to bulk operations, (5) Only proceed with irreversible bulk operations after BOTH verifications pass.",
      "helpful": 2,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00250",
      "content": "[strat-00014] For verification steps that extract and validate values from API responses: (1) Always inspect the target API's specification BEFORE writing field access code, (2) Document the exact field structure from the schema (flat vs. nested), (3) Test field access on a sample response to confirm the path is correct, (4) Add explicit null/None checks after field access to catch cases where the wrong path silently returns None, (5) Only after confirming field access returns expected values, proceed with verification assertions. This prevents verification from passing incorrectly when field access silently fails.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00253",
      "content": "[strat-00014] For multi-app financial workflows requiring authentication to multiple services: (1) Authenticate to each service independently before making any API calls to that service, (2) Do not assume a single authentication session covers multiple apps or services, (3) Verify successful authentication to each service before proceeding to API operations on that service. This ensures isolation of service-specific credentials and prevents cross-service authentication failures.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00257",
      "content": "[str-00040] For multi-app workflows requiring operations across different services (e.g., file system + Venmo): (1) Authenticate to each service independently before making API calls, (2) Do not assume authentication credentials transfer between services, (3) Complete all data extraction from the first service before switching to the second service, (4) Store extracted data (e.g., roommate IDs, bill amount) in memory before initiating operations in the second service. This ensures service-specific authentication requirements are met and data is complete before irreversible operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00258",
      "content": "[str-00049] For API calls with required parameters (e.g., Venmo payment requests with description field): (1) Identify all required and optional parameters from the API specification BEFORE constructing the request, (2) Validate that all required parameters are available and correctly formatted, (3) Match parameter values exactly to task specifications (e.g., description text must be verbatim), (4) Construct the request with specification-first approach, ensuring no parameters are omitted or mismatched. This prevents parameter validation errors and ensures API calls succeed on first attempt.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00260",
      "content": "[strat-00014] For multi-app workflows requiring authentication: (1) Always follow specification-first approach - call show_api_doc() BEFORE making any API call to verify parameter names, required parameters, return types, and authentication requirements, (2) Understand that authentication credentials (access_token, etc.) must be explicitly obtained and passed to each API call - they are NOT automatically managed or inherited across calls, (3) Obtain credentials from supervisor.show_account_passwords() or equivalent supervisor APIs, not from non-existent credential retrieval APIs on target services, (4) Store authentication tokens in variables within the execution context and pass them explicitly to all subsequent API calls that require authentication, (5) Before each new API call, consult show_api_doc() to verify the exact parameter names and whether authentication is required.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00273",
      "content": "[str-00026] For multi-step tasks that logically form a single cohesive workflow (e.g., authenticate, retrieve state, filter items, apply bulk updates, verify): (1) Consolidate all steps into a single action field with executable Python code when possible, rather than spreading across multiple steps, (2) Only split into multiple steps if intermediate manual verification or decision-making is required, (3) Within a single action field, use comments to separate logical phases (e.g., '# Phase 1: Authentication', '# Phase 2: Retrieve state'), (4) Ensure all variables created in one phase are available to subsequent phases within the same action execution. This pattern maximizes efficiency and prevents variable persistence loss.",
      "helpful": 3,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00277",
      "content": "[str-00163] For multi-service workflows requiring API calls to different services: (1) Authenticate to each service independently before making any API calls to that service, (2) Retrieve service API specifications or documentation before constructing requests, (3) Store authentication tokens/credentials in variables for reuse across multiple API calls to the same service, (4) Do not assume a single authentication step covers all services; each service may require separate login or credential exchange.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00282",
      "content": "[str-00033] For any API call to an unfamiliar or newly encountered endpoint: (1) Call show_api_doc() with the API name and method before making the actual API call, (2) Document the exact parameter names, types (required vs optional), and response structure from the specification, (3) Verify that your intended parameters and response field access match the specification exactly, (4) Only after specification review, proceed with the actual API call. This specification-first approach prevents parameter mismatches and response schema assumptions.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00283",
      "content": "[str-00034] For multi-step workflows with state modifications: (1) Ensure every step has both a thought field (reasoning) AND an action field (executable code), (2) Do not leave steps with only reasoning and empty action fields, as this breaks the execution chain and prevents operations from completing, (3) Verify that variables created in earlier action fields are referenced and used in subsequent action fields, (4) Test variable persistence by explicitly printing or using variables in the action field to confirm they were instantiated in prior steps.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00294",
      "content": "[str-00207] In each step's action field, write actual executable code (not pseudocode or explanation) that completes the phase and creates output variables. Use the thought field for reasoning, planning, or pseudocode sketches. Only code executed in the action field persists as variables for subsequent steps.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00295",
      "content": "[str-00208] For multi-step task workflows with sequential dependencies: (1) Ensure each step's action field contains runnable Python code that creates variables needed by the next step, (2) Verify variable names are consistent across steps, (3) Include a terminal completion step that calls apis.supervisor.complete_task(answer=...) with the final result. Tasks are not complete until the completion step is executed.",
      "helpful": 2,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00296",
      "content": "[str-00209] Before calling unfamiliar APIs in executable code, first retrieve the API specification via show_api_doc() to verify parameter names, types, response structure, and required fields. This prevents parameter mismatches, type errors, and incorrect field access in subsequent code steps.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00300",
      "content": "[str-00033] For multi-step task workflows involving authentication, search, enrichment, filtering, and action execution: (1) Always call show_api_doc() before using any unfamiliar API to understand response structure and required parameters, (2) Execute phases sequentially: authentication → search/retrieval → enrichment (fetch detailed fields) → filtering/comparison → primary action → verification, (3) Verify each phase succeeded before proceeding to the next phase (e.g., confirm access_token was extracted correctly, confirm target resource exists, confirm enriched fields are present) to catch errors early and enable quick recovery.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00305",
      "content": "[str-00033] [strat-00014] For tasks with irreversible completion marking (e.g., apis.supervisor.complete_task()): (1) Treat the completion marking step as a critical gate that should only be crossed after all prior steps are verified, (2) Implement a two-tier verification: first, inspect the primary action's API response for success indicators; second, optionally query system state to confirm persistence, (3) Document the success criteria explicitly (e.g., 'Response must contain message with \"now playing\"'), (4) Only proceed to completion marking if all verification criteria are met, (5) If verification fails, raise an error or return a diagnostic message rather than silently completing. This prevents marking tasks done when underlying actions failed, which is irreversible in the system.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00309",
      "content": "[str-00049] For search-dependent tasks where multiple candidates may match the query: (1) Execute the search query and retrieve all results from the first page, (2) Inspect all candidates and their metadata (title, artist, release_date, genre, etc.), (3) If ambiguity exists (multiple plausible matches), apply disambiguation logic: re-search with a more specific query, filter by additional criteria (artist name, date range, genre), or compare metadata to task context, (4) Select the candidate that best matches the task's implied intent, (5) Document the selection and verification criteria explicitly before proceeding to dependent operations. This prevents incorrect entity selection when search results are ambiguous.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00312",
      "content": "[strat-00301] For multi-step workflows with sequential dependencies: (1) In the 'thought' field, reason about task structure, plan phases, and identify data dependencies between steps, (2) In the 'action' field, write complete executable Python code that instantiates all necessary variables and performs the actual work (API calls, filtering, transformations), (3) Verify success of each phase within the action code before proceeding to the next phase, (4) Document intermediate results (counts, filtered items, accepted requests) as variables within the action code, (5) After the primary action completes and is verified, execute a final action step that calls apis.supervisor.complete_task(answer=summary) to mark the task complete in the system. This pattern ensures variables persist across steps and tasks are properly marked as complete.",
      "helpful": 3,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00313",
      "content": "[strat-00302] For task workflows that accept, modify, or delete items: (1) Complete all identification, filtering, and verification of target items BEFORE executing any irreversible modifications, (2) Execute all modifications (accept requests, update state, delete items) within a single action step where variables are instantiated and available, (3) Verify the modifications succeeded by re-querying or checking response status codes within the same action step, (4) Document counts at each checkpoint (total items identified, total items modified, total verified), (5) Only after all verification passes, execute the final completion marking step (apis.supervisor.complete_task()). This prevents incomplete modifications and ensures the system state is consistent with task expectations.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00317",
      "content": "[str-00033] When consolidating code from multiple successful execution steps: (1) Do not rewrite code from assumptions; instead, extract the exact working code pattern from the earlier successful steps, (2) If the same API was called successfully in a prior step (e.g., step 6), reference that execution and copy its code structure rather than reimplementing, (3) Before consolidating, call apis.api_docs.show_api_doc() to verify the response structure matches your code's expectations, (4) Test the consolidated code incrementally by executing each section and confirming success before proceeding to the next section, rather than writing the entire consolidated block and testing it all at once. This prevents regression errors where code that worked in isolation fails when consolidated due to forgotten details or changed assumptions.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00321",
      "content": "[str-00033] For bulk operations with qualifier-constrained scope (e.g., 'accept payment requests from coworkers and friends'): (1) Identify the qualifier and extract the relationship type or entity criteria (e.g., 'coworkers and friends'), (2) Resolve target entities BEFORE retrieving the full dataset by using relationship-based search (e.g., apis.phone.search_contacts(relationship='coworker')) or other entity identification methods, (3) Retrieve the full dataset with exhaustive pagination, (4) Filter the dataset to only items matching the target entities using OR logic (if multiple relationship types, a transaction matches if the participant is in ANY of the target entity lists), (5) Execute the bulk operation only on the filtered subset, (6) Verify that operations were applied only to filtered items, not to the entire dataset. This pattern ensures qualifier-constrained operations are scoped correctly.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00326",
      "content": "[str-00163] For irreversible or final API calls (task completion, account termination, data deletion, or any operation that cannot be undone): (1) Before calling the API, retrieve its specification using apis.api_docs.show_api_doc(app_name='[service]', api_name='[operation]'), (2) Document the exact parameter names, data types (string, dict, list, etc.), and expected format from the specification, (3) Construct the request payload to match the specification exactly—do not assume parameter names or formats, (4) Execute the API call with the specification-verified payload, (5) Inspect the response for success indicators (error fields, status codes, success messages, or confirmation fields), (6) Only mark the workflow complete after the response confirms the operation persisted. This prevents silent failures, parameter mismatches, and ensures the system records the final state correctly.",
      "helpful": 2,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00331",
      "content": "[strat-00014] For superlative tasks (finding most/least played, most/least liked, highest/lowest rated items, etc.): (1) Call show_api_doc() to inspect the target API specification and identify ALL pagination parameters (page_index, page_limit, next_page, offset, limit, etc.), (2) Document the pagination mechanism (offset-based, cursor-based, etc.), (3) Implement exhaustive pagination BEFORE any filtering or comparison operations, (4) Accumulate all results across all pages into a single collection, (5) Only after pagination is complete, apply deduplication, enrichment, filtering, and comparison functions to the COMPLETE dataset. This ensures no items are missed due to pagination boundaries, which is the most common cause of incorrect superlative answers.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00336",
      "content": "[str-00033] For multi-step workflows with verification (e.g., follow artists, then verify all were followed), establish formal checkpoints: (1) Before any field extraction, retrieve and document the API specification via show_api_doc(), (2) Execute the primary operation (e.g., follow artists), (3) Implement exhaustive pagination in verification (not just first page), (4) Use field names directly from the retrieved spec, (5) Compare complete datasets before and after, (6) Only mark the task complete after all verification passes. This checkpoint pattern prevents silent failures (wrong field names returning None) and false negatives (incomplete pagination) that can mask incomplete operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00340",
      "content": "[str-00033] [strat-00014] For all API field access operations, especially in verification logic: (1) Always consult the API specification via show_api_doc() BEFORE attempting to access response fields, (2) Document the exact field names, data types, and response structure from the specification, (3) Write field access code using only the verified field names, (4) Be aware that incorrect field names in Python/code silently return None without raising exceptions, which is particularly dangerous in verification steps where None values can indicate incomplete operations but may go unnoticed if not explicitly checked, (5) Test field access on a sample API response before applying to bulk verification. This applies universally across all API calls and all response structures, not just unfamiliar endpoints.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00344",
      "content": "[str-00033] [strat-00014] For multi-participant filtering workflows (e.g., 'transactions involving any of [entities]'): (1) At the entity identification stage, document the count of target entities retrieved, (2) At the pagination stage, document the total transaction count retrieved, (3) At the filtering stage, document the count of transactions matching the OR logic filter, (4) At the operation stage, document the count of operations executed, (5) At the verification stage, document that re-queried results match the filtered count. These intermediate counts enable detection of silent failures such as incomplete pagination, incorrect filter application, or operations that did not persist.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00346",
      "content": "[str-00033] For multi-phase workflows with data dependencies (e.g., retrieve → deduplicate → process → export → verify → complete): (1) Consolidate all executable code into a single 'action' field with clear phase comments, rather than splitting across multiple steps, (2) This ensures variables created in phase 1 (e.g., song_list) persist through phases 2-6 without NameError, (3) Use comments to separate logical phases (e.g., '# Phase 1: Retrieve all songs with pagination'), (4) Write all code in one 'action' field even if the workflow is complex, to maintain variable scope and execution continuity, (5) Only use multiple steps if a step requires human input or external verification that blocks subsequent steps. This pattern prevents variable loss and ensures complete workflow execution.",
      "helpful": 4,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00347",
      "content": "[str-00034] For tasks requiring task completion marking (e.g., 'export data then terminate account'): (1) Identify all irreversible or completion-marking operations (e.g., apis.supervisor.complete_task(), account termination), (2) Ensure these operations are written as executable code in the 'action' field, not pseudocode in the 'thought' field, (3) Place the completion-marking operation at the end of the consolidated 'action' field after all data aggregation and verification is complete, (4) Verify in the 'observation' field that the completion API actually executed (check for success response or absence of errors), (5) If the completion API is only written in 'thought' and not executed in 'action', the task will remain incomplete and final_answer will be null. This is a hard requirement for task closure.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00350",
      "content": "[str-00XXX] For multi-phase workflows with data dependencies (e.g., login → retrieve → deduplicate → format → write → verify → terminate): (1) Consolidate all executable code into a single 'action' field with clear phase comments, (2) Define all variables (access_token, collections, output_data, etc.) within the same action block to maintain scope across phases, (3) Structure the code as sequential phases with intermediate outputs documented in print statements or return values, (4) Avoid splitting executable code across multiple action fields, as variables created in earlier steps will not persist (NameError). A single consolidated action ensures all phases share variable scope and execute atomically.",
      "helpful": 1,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00355",
      "content": "[str-00033] For final/irreversible API calls (task completion, account termination, deletions, permanent state changes): (1) Apply specification-first approach—call show_api_doc() to retrieve the endpoint specification BEFORE constructing the request, (2) Document exact parameter names, types, required fields, and expected response structure from the specification, (3) Construct the request to match the specification exactly, avoiding assumptions about parameter names or types, (4) Execute the call and inspect the response for both success indicators AND verification that critical parameters were properly registered (e.g., if submitting an answer, verify the answer field is populated in the response or queryable in system state), (5) Never proceed assuming silent success based only on a generic success message—always verify critical output fields. This pattern prevents silent failures where operations appear to complete but critical data is not registered.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00358",
      "content": "[strat-00014] For multi-step task workflows with execution constraints: (1) Consolidate all executable code into the 'action' field—do not write code in the 'thought' field, which is for reasoning only. (2) Use the 'thought' field to write high-level reasoning, pseudocode, and planning. (3) Verify each step's success by checking the 'observation' field for non-empty results before proceeding to the next step. (4) Maintain variable scope by keeping related executable steps in the same 'action' field when possible. (5) End all task workflows with an explicit completion step calling apis.supervisor.complete_task(answer=...) to close the task in the system.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00362",
      "content": "[str-00033] For workflows involving unfamiliar APIs or services (e.g., first-time interaction with simple_note, supervisor, or other external services): (1) Call apis.api_docs.show_api_doc(app_name='[service_name]', api_name='[api_name]') BEFORE attempting any API call to verify exact parameter names, types, required fields, and authentication requirements. (2) Document the specification (parameter names, response schema, error codes) in your reasoning. (3) If a subsequent API call returns a 422 validation error, immediately re-inspect the API spec to confirm parameter names match exactly—do not assume parameter names from task context or similar APIs. (4) Only proceed with the actual API call after specification verification confirms all parameters are correct.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00363",
      "content": "[str-00034] For multi-step workflows with sequential dependencies (e.g., authenticate, then query, then parse, then submit): (1) Consolidate ALL executable Python code (API calls, variable assignments, data transformations) into a single action field with clear phase comments (e.g., '# Phase 1: Login', '# Phase 2: Query', '# Phase 3: Parse'). (2) Use the thought field ONLY for reasoning and planning; do NOT place executable code or pseudocode in the thought field. (3) Ensure all variables created in Phase 1 (e.g., access_token, session_id) are available for use in Phase 2 and Phase 3 within the same action field. (4) If a workflow requires multiple independent action fields, explicitly pass outputs from one field to the next via documented variable names. This pattern prevents variable scope loss and execution failures caused by code in thought fields not being executed.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00364",
      "content": "[str-00035] For authentication workflows in multi-step tasks: (1) Call the login or authentication API (e.g., apis.simple_note.login()) using the specification-verified parameter names. (2) Extract the authentication token or session identifier from the API response (e.g., response['access_token'] or response['session_id']). (3) Pass the extracted token to all subsequent authenticated API calls by including it in the request headers or as a parameter (verify the exact field name via show_api_doc()). (4) Document the token extraction and passing explicitly in code comments to ensure subsequent steps can reference the correct variable name. (5) Verify the authentication was successful by checking for error indicators in the login response before proceeding to authenticated calls.",
      "helpful": 0,
      "harmful": 0,
      "section": "strategies_and_hard_rules"
    },
    {
      "id": "str-00374",
      "content": "[str-00033] For irreversible completion steps in multi-step workflows (e.g., marking a task complete via apis.supervisor.complete_task()): (1) Build and test the completion summary string independently before passing it to the completion API—use simple string concatenation or basic f-strings, never nested f-strings or complex expressions, (2) Call apis.supervisor.show_api_doc(app_name='supervisor', api_name='complete_task') to verify the exact parameter names and types expected by the completion API, (3) Execute the completion call with explicit error handling and capture the response, (4) Verify the task was actually marked complete by calling apis.supervisor.show_active_task() and confirming the status changed to 'complete' or the active task is None, (5) Recognize that syntax errors in the completion step are catastrophic—they prevent closing the task even though all prior operations succeeded. The completion step is the system gate; treat it with defensive programming and post-execution verification.",
      "helpful": 0,
      "harmful": 0
    }
  ],
  "verification_checklist": [
    {
      "id": "ver-00012",
      "content": "[ver-00005] After completing bulk state-based operations (create/update), verify the final state by: (1) Sampling a subset of modified items and re-querying their current state, or (2) Re-querying all items if the collection is small, (3) Confirming that items with no prior state now have the new state, (4) Confirming that items with prior state below the target threshold have been updated to the target state.",
      "helpful": 9,
      "harmful": 1,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00015",
      "content": "[ver-00013] When verifying bulk state-based operations with multiple independent requirements or constraints, ensure verification covers ALL requirements and constraints across ALL items in scope, not just the primary task objective. For example, if the task requires both 'rate non-liked songs at 1-star' AND 'ensure no song has rating > 1', verify both conditions across the entire song library, including liked songs with existing ratings.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00017",
      "content": "[ver-00016] When a task applies operations only to a filtered subset of items (e.g., 'rate only liked songs'), verify not only that items within the subset were correctly modified, but also that items OUTSIDE the subset (e.g., non-liked songs) were NOT unintentionally modified. Sample or re-query non-target items to confirm they remain in their original state.",
      "helpful": 11,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00023",
      "content": "[ver-00018] After filtering items by multi-participant OR logic, verify the filter was applied correctly by: (1) Sampling filtered items and confirming that each item has at least one participant matching the filter criteria, (2) Sampling items that were NOT included and confirming that none of their participants match the filter criteria, (3) Ensuring the verification covers both the primary participant field and all secondary participant fields to confirm OR logic was correctly applied.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00026",
      "content": "[ver-00024] When verifying bulk operations on paginated data sources, confirm pagination completeness by: (1) Documenting the total number of pages retrieved and total items accumulated, (2) Confirming that the final page returned fewer items than page_limit (or was empty), indicating pagination was exhausted, (3) Spot-checking that items from the last page(s) match the expected date/time range or other temporal filters to confirm no pages were skipped, (4) Re-querying the paginated API with a different page_limit to ensure the total item count remains consistent across different pagination configurations.",
      "helpful": 40,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00028",
      "content": "[ver-00027] For bulk state-modification operations, apply sample size thresholds based on collection size: (1) If ≤20 items modified, verify 100% (all items) or minimum 67% if time-constrained, (2) If 21-100 items modified, verify minimum 50%, (3) If >100 items modified, verify minimum 10-20% random sample. Document the sample size and verification rate explicitly. Always include out-of-scope sample verification per [ver-00017] to confirm filter correctness.",
      "helpful": 8,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00029",
      "content": "[ver-00027] When aggregating data from multiple paginated sources into a file output before performing irreversible operations (e.g., account deletion): (1) After each pagination loop, explicitly verify that the final page returned fewer items than page_limit or was empty, confirming pagination exhaustion, (2) Document the total item count from each source before deduplication, (3) Document the total unique item count after deduplication, (4) Sample and re-read the output file to confirm headers, formatting (e.g., | separators), and data integrity, (5) Only proceed to irreversible operations after confirming all verification steps pass.",
      "helpful": 3,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00034",
      "content": "[ver-00027] Before performing irreversible operations that depend on file output (e.g., account termination after creating a backup file): (1) Create the file with the specified format, (2) Read a sample of the output file (at least the header row and first few data rows), (3) Compare the sampled output against all formatting requirements in the task specification (delimiters, headers, field separators, encoding), (4) Confirm format compliance explicitly before proceeding to the irreversible operation. This prevents data loss or account deletion due to incorrect file format.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00037",
      "content": "[ver-00027] For tasks combining paginated data aggregation with irreversible operations (e.g., account termination), establish a formal verification checkpoint before the irreversible operation: (1) Confirm pagination exhaustion for each source with explicit documentation, (2) Log intermediate counts (per-source totals, combined totals, unique count after dedup), (3) Create and read back output file to verify format (headers, delimiters, field count, line count), (4) Explicitly state 'All verification checks passed. Safe to proceed to irreversible operation.' (5) Only then execute the irreversible operation. This pattern prevents data loss due to incomplete backups preceding account termination.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00042",
      "content": "[ver-00027] For multi-app workflows with sequential dependencies, verify completion at each stage before proceeding: (1) After authentication, confirm access tokens are valid by making a test API call or checking token metadata, (2) After search/query operations, confirm the target entity was located before attempting retrieval, (3) After data extraction from unstructured content, verify that extracted data matches the expected format and completeness (e.g., all movie titles were extracted, none are missing), (4) After formatting output data, verify it matches task specifications (e.g., comma-separated, no extra whitespace), (5) Only after all verification passes, execute the final action (e.g., send message).",
      "helpful": 32,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00047",
      "content": "[ver-00027] For multi-app workflows with data extraction and communication: (1) Verify authentication succeeded for each service by confirming a successful API response, (2) Verify data extraction by confirming the parsed list contains expected items and no parsing errors occurred, (3) Verify formatting by confirming the output matches task specifications (e.g., comma-separated, no duplicates), (4) Verify delivery by confirming the communication API (e.g., send_sms) returns a success response. Complete all verification steps before concluding task completion.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00050",
      "content": "[ver-00027] When calling send_text_message or similar messaging APIs, verify success by: (1) Checking that the response status is 200 or 201 (not 422 or other validation errors), (2) Confirming the response includes a text_message_id or similar identifier, (3) Optionally retrieving the sent message to confirm the content matches the formatted output exactly, (4) Only marking the task complete after successful delivery confirmation.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00053",
      "content": "[ver-00027] [ver-00028] When aggregating paginated results from multiple entities with OR logic, verify aggregation completeness by: (1) Documenting the number of items retrieved per entity (e.g., 'Coworker A: 3 transactions, Coworker B: 2 transactions, Coworker C: 0 transactions'), (2) Documenting total items across all entities before and after deduplication, (3) Confirming pagination exhaustion for EACH entity independently (not just the last one), (4) For small aggregated collections (≤20 items), verify 100% of items; for larger collections, sample at least 10% or 20 items, whichever is larger.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00059",
      "content": "[ver-00027] For bulk filter-and-modify operations, implement explicit count documentation at key checkpoints: (1) Total items retrieved from paginated source, (2) Total items matching filter criteria, (3) Total items excluded by filter, (4) Total items successfully modified. Document these counts in order and confirm they sum correctly. This enables detection of silent partial failures.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00060",
      "content": "[ver-00028] When verifying bulk modifications to content fields (e.g., comments, messages, descriptions), retrieve the actual content for a sample of modified items (at least 3) and confirm it matches the specification exactly. Do not rely on presence indicators (e.g., comment_count > 0) as a proxy for correctness; always inspect the actual content.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00064",
      "content": "[ver-00025] Before implementing verification logic for bulk operations that modify external state (adding comments, liking items, updating fields): (1) Call show_api_doc() for the retrieval API (e.g., show_transaction_comments, show_song_details) to inspect the response schema, (2) Document the exact field names and types returned in the response object, (3) Verify that your verification code accesses fields using the documented names (e.g., comment.get('comment'), not comment.get('text')), (4) Test field access on a sample response before running full verification. This prevents verification failures caused by field name mismatches.",
      "helpful": 6,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00065",
      "content": "[ver-00026] When verifying bulk operations that add or modify content (comments, reviews, notes), verify the actual content value matches expectations, not just the presence of the field. For example, after adding a comment, check that comment.get('comment') == expected_comment_text, not just that comment_count > 0. Presence checks miss cases where the wrong content was added or the field is empty.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00080",
      "content": "[ver-00025] When verifying payment request or communication operations (e.g., Venmo payment requests): (1) Retrieve the list of sent requests or communications using the appropriate query API (e.g., apis.venmo.show_payment_requests()), (2) Confirm that the count of created requests matches the expected number of recipients, (3) Sample or verify all requests to confirm they contain the correct recipient identifiers, description text, and amount fields, (4) Confirm that requests were not sent to participants who already completed the transaction (i.e., verify the exclusion logic was applied correctly). This ensures payment requests were created only for intended recipients with correct details.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00084",
      "content": "[ver-00027] When extracting and matching data from unstructured sources against API search results, verify matches by: (1) Documenting the matching criteria used (e.g., 'first name from note matched to API result by name prefix'), (2) For each match, confirming supporting metadata exists (e.g., friendship status is not null, email domain matches, contact appears in multiple search contexts), (3) Flagging low-confidence matches where only one result exists or multiple plausible results exist, (4) Sampling a subset of matches to confirm they are semantically correct before proceeding to dependent operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00089",
      "content": "[ver-00027] When verifying bulk operations that depend on entity matching (e.g., payment requests to matched coworkers), confirm match accuracy by: (1) Sampling a subset of matched entities and re-querying their metadata (e.g., contact relationship type, email) to verify the match was correct, (2) Confirming that the number of matched entities equals the number of target entities identified from the source data (e.g., coworkers listed in notes), (3) Checking for any unmatched entities that should have been included but were not, indicating potential matching failures with common names or missing entities.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00092",
      "content": "[ver-00027] When aggregating transaction or similar data across multiple target entities, verify results by: (1) Documenting the count of target entities identified (e.g., '3 roommates'), (2) Documenting the total number of transactions retrieved via pagination (e.g., '45 total transactions across all pages'), (3) Documenting the count of transactions that matched the filter criteria (e.g., '12 transactions sent to any roommate'), (4) Providing a breakdown by target entity (e.g., sum by each roommate) to confirm no entity was missed, (5) Confirming the final aggregated value (e.g., total sum) matches the sum of per-entity breakdowns.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00099",
      "content": "[ver-00027] When filtering transactions or events by date range, verify temporal correctness by: (1) Sampling retrieved items and confirming their created_at or timestamp fields fall within the expected date range specified in the task, (2) Checking that the earliest item's date is >= the min_created_at filter used, (3) If a max_created_at filter was applied, confirming items do not exceed that boundary, (4) Documenting the actual date range of retrieved items (e.g., 'Retrieved 45 transactions from 2024-03-01 to 2024-12-15') and confirming it matches the task's temporal scope. This prevents silent errors where the wrong year or month was used in filtering.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00102",
      "content": "[ver-00027] For multi-step workflows with sequential dependencies on entity data (e.g., extract artists from songs, then follow those artists, then verify they were followed): (1) After extracting entities in step N, verify the extraction by sampling responses and confirming the expected field structure before proceeding to step N+1, (2) After completing step N+1 (e.g., follow operations), verify intermediate results by querying the modified state (e.g., show_following_artists()), (3) Cross-validate that entities in the intermediate result match entities extracted in step N by comparing both ID and metadata fields (e.g., artist name), (4) Document any mismatches and investigate whether they indicate incomplete operations or schema inconsistencies. Do not proceed to final verification until intermediate cross-validation passes.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00112",
      "content": "[ver-00027] When implementing categorical field filtering (e.g., genre filtering), verify filter correctness before applying to the full dataset by: (1) Testing the filter logic on a sample of 3-5 items with known categorical values, (2) Confirming that items matching the filter criteria (e.g., songs with 'indie' in the genre field) are correctly identified, (3) Confirming that items NOT matching the criteria are correctly excluded, (4) Spot-checking edge cases such as compound values (e.g., 'indie rock'), case variations (e.g., 'Indie' vs. 'indie'), and missing/null values.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00117",
      "content": "[ver-00027] When aggregating data from file systems with time-bounded scope (e.g., 'total for this year'): (1) Document expected item count based on scope (e.g., 12 months for full year), (2) Document actual items found (e.g., 4 months: Jan-Apr), (3) Explicitly list which periods are missing (e.g., May-Dec), (4) Calculate coverage percentage (e.g., 4/12 = 33%), (5) Do NOT provide final aggregation as 'total for the year' if coverage is incomplete; instead, provide partial total with coverage notes (e.g., 'Jan-Apr total: $368.0; May-Dec data missing or unavailable'). This prevents misrepresentation of partial data as complete year aggregations.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00126",
      "content": "[ver-00027] For time-bounded aggregation tasks with multiple data sources (e.g., 'total for this year' from multiple files), verify temporal scope completeness by: (1) Documenting the expected temporal range (e.g., Jan-Dec 2023 for 'this year'), (2) Documenting the actual periods found (e.g., files for Jan-May exist), (3) Documenting the actual periods successfully parsed (e.g., Jan-Apr parsed, May binary format unreadable), (4) Calculating coverage percentage (successfully parsed periods / expected periods), (5) Explicitly listing missing periods (Jun-Dec not found), (6) Before providing the final aggregated value, confirm whether the result represents complete coverage or partial coverage. If partial, document this clearly in the final answer.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00127",
      "content": "[ver-00028] When providing aggregated results from incomplete data sources, use explicit coverage notation: (1) If coverage is 100% (all expected periods found and parsed), present result as: '[value] (complete: all [N] periods covered)', (2) If coverage is partial (some periods missing or unparseable), present result as: '[value] (partial: [M]/[N] periods covered, [X]% coverage; missing periods: [list])', (3) For each missing period, document the reason (file not found, file exists but format unreadable, etc.), (4) Never present a partial aggregate without explicit coverage documentation, as this misleads the user into assuming completeness.",
      "helpful": 0,
      "harmful": 1,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00130",
      "content": "[ver-00028] When completing aggregation tasks with time-bounded or partial data sources, verify that the final answer format matches the expected output schema and task requirements: (1) If the task asks a direct question (e.g., 'What is the total cost?'), provide the answer in the format matching ground truth expectations (e.g., numeric value), (2) Maintain coverage documentation (missing periods, percentage coverage, data completeness) in internal verification logs, (3) Only include coverage qualifications in the final answer if the task explicitly requests documentation of completeness or if omitting them would create ambiguity, (4) For high-stakes or irreversible operations, prioritize coverage documentation in the final output to prevent misrepresentation of partial data.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00134",
      "content": "[ver-00027] When verifying file move or bulk reorganization operations, confirm both positive and negative outcomes: (1) Sample or verify all items in each destination container match the expected categorization criteria (e.g., files created in February are in the Petra directory), (2) Confirm the source container is now empty or contains only items that were correctly excluded from reorganization, (3) Verify file counts: total files in all destination containers should equal the initial file count in the source container, (4) Spot-check file metadata (e.g., creation timestamps) in destination containers to confirm categorization accuracy.",
      "helpful": 4,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00137",
      "content": "[ver-00027] When verifying metadata-driven reorganization tasks (e.g., files moved into categorized directories), verify both positive and negative outcomes: (1) Confirm that items are present in their correct destination containers by sampling across all categories and inspecting enriched metadata to validate categorization accuracy (e.g., verify a Rome file has created_at in March), (2) Confirm that the source container is empty or contains only items that do not match any categorization criteria, ensuring complete reorganization, (3) Verify that item metadata was preserved correctly during reorganization (e.g., created_at timestamps remain unchanged if retain_dates=True was used).",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00144",
      "content": "[ver-00027] After completing file reorganization operations: (1) Verify that the source directory is empty or contains only expected remaining files, (2) Verify that each destination subdirectory contains the correct number of files for its category, (3) Sample files from each destination and confirm their original file names are intact, (4) Sample files from each destination and confirm their original metadata (created_at timestamps) is preserved, (5) Document total file count before and after to confirm no files were lost or duplicated during reorganization.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00147",
      "content": "[ver-00027] When verifying bulk removal operations across multiple independent sources, verify each source independently: (1) Re-query the library/primary collection to confirm target items are absent, (2) Re-query each secondary collection (e.g., each playlist) to confirm target items are absent from that collection, (3) Confirm that non-target items remain present in all sources where they were not removed, (4) Document removal counts for each source separately to confirm completeness. This prevents missing removals from secondary sources due to incomplete verification.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00150",
      "content": "[ver-00027] When performing bulk removal operations across multiple sources with deduplication (e.g., removing songs from both library and playlists), verify deduplication correctness by: (1) Documenting the count of items from each source before deduplication, (2) Documenting the count of unique items after deduplication, (3) Confirming that removed items no longer appear in any source (library, all playlists, etc.) by re-querying each source exhaustively, (4) Spot-checking that items appearing in multiple sources were removed from all instances, not just the first occurrence.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00155",
      "content": "[ver-00027] [ver-00025] For multi-step workflows with a final action step (e.g., play_music, send_payment): (1) After executing the action API call, inspect the response for success indicators (e.g., status='success', error field is null/absent), (2) Document the response and confirm the action was accepted by the API, (3) Verify that the system state changed as expected (e.g., music is now playing, playlist is active), (4) Only after action verification passes, proceed to execute the task completion step (apis.supervisor.complete_task()) with a descriptive summary, (5) Confirm the completion call succeeds before considering the task fully resolved.",
      "helpful": 23,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00157",
      "content": "[ver-00042] For playlist or collection creation tasks with duration/size requirements: (1) After collecting all candidate items (songs, episodes, etc.), calculate the total duration by summing individual item durations, (2) Verify the total duration meets or exceeds the specified requirement (e.g., 90 minutes for a workout), (3) If total duration is insufficient, implement pagination or additional search to collect more items until the requirement is met, (4) Document the final item count and total duration before proceeding to add items to the playlist or execute the primary action. This prevents attempting to play insufficient content.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00170",
      "content": "[ver-00027] When verifying bulk removal operations on multi-source collections (songs and albums): (1) Re-query `show_song_library()` with exhaustive pagination and confirm all songs in the removal set are absent, (2) Re-query `show_album_library()` with exhaustive pagination and confirm all albums in the removal set are absent, (3) Sample songs and albums from the keep set and confirm they are still present in their respective libraries, (4) Verify that removal counts match the documented filtering counts (songs to remove, albums to remove) to confirm no unintended items were removed.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00179",
      "content": "[ver-00027] After performing file system operations with compound parameters (e.g., compress_directory with delete_directory=True), verify the final state by: (1) Re-querying the target directory structure to confirm compressed artifacts exist at the expected location, (2) Confirming that source directories marked for deletion no longer exist, (3) Spot-checking file counts or sizes in the compressed artifacts to ensure data integrity, (4) Documenting the initial state, operations performed, and final state for audit purposes.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00183",
      "content": "[ver-00027] Before executing irreversible file system operations (compress with delete, bulk delete, move with overwrite), verify the initial directory structure: (1) List the source directory at the path specified in the task description, (2) Confirm that the directory names and nesting depth match the task specification (e.g., if task says 'subdirectories of ~/photos/', confirm they are DIRECT children, not nested deeper), (3) If the observed structure differs from the task specification, document the discrepancy and investigate before proceeding, (4) Explicitly confirm the source and destination paths in writing before executing the operation. This prevents data loss from operating on unintended locations.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00187",
      "content": "[ver-00027] Before executing file system operations involving irreversible actions (compress with delete, bulk delete, move with overwrite), verify directory structure alignment by: (1) Listing the source directory with recursive=false to inspect only DIRECT children, (2) Documenting the observed structure explicitly (e.g., directory names, nesting depth, total count of items found), (3) Comparing the observed structure against the task description's implied structure (e.g., does the task expect direct children or nested subdirectories?), (4) Confirming that the source paths for the irreversible operation match the task's intent, (5) If discrepancies exist between observed and expected structure, documenting them explicitly before proceeding (or requesting clarification). This verification must complete BEFORE any irreversible operations are initiated.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00191",
      "content": "[ver-00027] When filtering items by multiple criteria (e.g., genre='classical' AND release_year='2023'), verify filter correctness by: (1) Confirming all target items (those included in the result) match ALL specified criteria, (2) Sampling excluded items and confirming that each excluded item fails at least one criterion, (3) For each criterion, verify that items matching that criterion but failing others are correctly excluded. This two-directional verification (target items match, excluded items correctly excluded) detects filter logic errors that would be missed by one-directional verification.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00192",
      "content": "[ver-00028] After completing multi-step data aggregation and filtering workflows, document the verification trail in the final answer by including: (1) Source count (total items from initial API), (2) Intermediate counts at each transformation stage (enriched, filtered, etc.), (3) Final count (items successfully added/modified), (4) Verification results (sample of target items confirmed matching criteria, sample of excluded items confirmed not matching criteria, final state verification). This documentation enables stakeholders to audit the result's completeness and correctness.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00204",
      "content": "[ver-00027] After completing bulk operations on filtered collections with multiple enrichment-based criteria, include verification counts in the completion summary: (1) Document total items retrieved from source API before filtering, (2) Document total items after enrichment, (3) Document count of items matching each individual filter criterion, (4) Document final count of items matching ALL criteria and added to the target resource, (5) Re-query the target resource (e.g., show_playlist()) to confirm final count matches the added count. This provides auditability and confidence in filter correctness.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00209",
      "content": "[ver-00027] Before proceeding to the next step in a multi-step workflow, verify that the current step's code executed successfully by: (1) Checking for exceptions (NameError, AttributeError, TypeError, etc.) in the action output, (2) Confirming that all expected output variables were created and contain non-empty data, (3) Spot-checking that the output matches the expected schema or format (e.g., list of song_ids should be a list, not a string), (4) If the step's code did not execute (e.g., action field contained only explanation), re-run the step with actual executable code before proceeding. Do not attempt to reference variables from a step that did not execute.",
      "helpful": 3,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00213",
      "content": "[ver-00027] When a task uses temporal scope qualifiers like 'played so far', 'to date', or 'up to now', verify the filtering boundary by: (1) Confirming the current position or state marker was correctly identified, (2) Sampling items at the boundary (current position and position + 1) to confirm the correct item is included and the next item is excluded, (3) Confirming no future/upcoming items beyond the boundary were included in the operation, (4) Documenting the total count of in-scope items and confirming it matches the expected range.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00217",
      "content": "[ver-00027] When verifying bulk operations using paginated aggregate or list APIs (e.g., show_liked_songs(), show_song_library()), confirm pagination exhaustion by: (1) Inspecting the API schema via apis.api_docs.show_api_doc() to identify pagination parameters, (2) Implementing a pagination loop and documenting total pages and total items retrieved, (3) Confirming the final page returned fewer items than page_limit (or was empty), (4) If pagination is not available or returns incomplete results, switch to item-level verification APIs to directly query target item state, (5) Document the verification method used and any fallbacks applied.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00220",
      "content": "[ver-00027] When filtering paginated results by entity criteria and then selecting an extremum (e.g., most recent payment to a specific person), explicitly document at verification: (1) 'Retrieved X total items across Y pages. Final page had Z items (< page_limit), confirming exhaustion.' (2) 'Filtered to [entity criteria]: N items found.' (3) 'Selected extremum by [field]: [value]. Timestamp/date: [date].' (4) 'Temporal alignment check: Does this [date] match the task's implied timeframe? [Yes/No/Unclear].' If unclear, note the discrepancy for manual review.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00224",
      "content": "[ver-00027] When completing Venmo transactions (payment sends or request approvals), verify transaction completion by: (1) Checking that the response includes a transaction_id field (not None/null), (2) Confirming the 'status' field in the transaction object is not None/null, indicating the transaction has been processed, (3) Optionally re-querying the transaction via apis.venmo.get_transaction(transaction_id) to confirm the status field reflects a completed state and the receiver's account reflects the fund transfer. Do not assume a transaction is complete based solely on the sender's view of the transaction.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00230",
      "content": "[ver-00027] When selecting a single item as 'the last' or 'most recent' from a filtered set, verify the selection by: (1) Confirming the extrema field (e.g., created_at) is present and non-null in the selected item, (2) Spot-checking that the selected item's timestamp is later than at least one other item in the filtered set (if multiple items exist), (3) Explicitly logging the extrema field value (e.g., 'selected request from 2024-01-15T10:30:00Z'), (4) Confirming this timestamp aligns with the task's implied recency context (e.g., 'recent accident' suggests a recent date).",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00233",
      "content": "[ver-00027] [ver-00025] After completing bulk deletion operations on filtered datasets: (1) Re-query using the exact same search criteria (filters, phone number, message type, etc.) that identified items for deletion, (2) Confirm the re-query returns zero results or an empty list, (3) Document the re-query response to confirm deletion completeness, (4) Only mark the task complete after verification confirms zero items remain matching the deletion criteria. This pattern ensures no items were missed due to pagination boundaries or deletion failures.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00235",
      "content": "[ver-00027] [ver-00025] After completing bulk DELETE operations on paginated data sources, verify deletion success by: (1) Re-querying the same data source using identical filter criteria that were used to identify deletion targets, (2) Confirming that the result set is empty or contains zero items, (3) Documenting the re-query result as proof that all target items were successfully deleted. This verification is essential for irreversible operations to confirm no items matching the deletion criteria remain.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00239",
      "content": "[ver-00028] [ver-00027] For bulk deletion operations on paginated data, verify completeness using a three-checkpoint pattern: (1) After pagination loop, confirm exhaustion by logging 'Final page had X items, page_limit was Y, exhausted: X < Y', (2) Before re-query verification, log intermediate deletion counts ('Deleted X text messages, Y voice messages'), (3) After re-query verification, explicitly log re-query results ('Re-query with original filter returned 0 items'). Document all three checkpoints in sequence; absence of any checkpoint indicates incomplete verification. This pattern ensures pagination was exhaustive, deletion was complete, and verification was performed with identical search criteria.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00241",
      "content": "[ver-00027] After exhaustive pagination on a filtered search or bulk retrieval operation, verify search/filter completeness by: (1) Documenting the total count of items returned by the filtered search, (2) If the result count is unexpectedly low (e.g., 1 classical artist out of a large music catalog), implement cross-validation by running an alternative query without the filter or with relaxed filter boundaries, (3) Manually filter the alternative result set using the original criteria and compare the count against the filtered search result, (4) If counts diverge, investigate the discrepancy before proceeding with bulk operations on the filtered results. This ensures search parameters are working as intended, not just that pagination was exhausted.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00245",
      "content": "[ver-00027] When verifying filtered search results before bulk operations, confirm search completeness separately from pagination exhaustion by: (1) Documenting pagination exhaustion (final page size < page_limit), (2) Re-running the search with a relaxed or absent filter to establish a baseline count of total matching items, (3) Manually counting how many baseline results meet the original filter criteria, (4) Comparing this manual count against the filtered search result count, (5) If counts diverge, investigating the discrepancy (e.g., testing boundary conditions, inspecting filter parameter documentation, sampling results via detail API) before proceeding, (6) Documenting the cross-validation results and confirming alignment before initiating bulk operations.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00249",
      "content": "[ver-00028] Before running verification logic that accesses response fields, validate the response schema structure by: (1) Inspecting the API specification via show_api_doc() for the verification endpoint, (2) Confirming the exact field names and nesting paths in the response schema, (3) Testing field access on a sample response to confirm the correct path (e.g., 'receiver.email' vs 'receiver_email'), (4) Explicitly checking that accessed fields return non-None values before proceeding with verification assertions. This prevents silent failures where incorrect field paths return None without raising exceptions.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00256",
      "content": "[ver-00027] After creating bulk financial transaction requests (e.g., Venmo requests to multiple recipients), verify by: (1) Confirming the number of requests created matches the number of recipients, (2) Sampling requests and re-querying to confirm each request has the correct recipient, correct amount (split amount, not total), and correct description matching the task specification exactly, (3) Confirming no duplicate requests were created for the same recipient, (4) Documenting the total amount requested across all requests equals the original bill amount.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00265",
      "content": "[ver-00027] After modifying unstructured content (e.g., updating note text, marking items as done), verify the modification persisted by: (1) Re-retrieving the resource via the appropriate read API (e.g., show_note()), (2) Confirming that the modified content (e.g., '[DONE]' marker) is present in the retrieved text, (3) Confirming that no unintended side effects or partial updates occurred (e.g., the entire content is intact, only the target item was modified). This prevents silent failures where update APIs accept requests but do not persist changes.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00267",
      "content": "[ver-00027] [ver-00025] For irreversible operations (task completion, deletions, account termination, state changes that cannot be undone): (1) Before executing the operation, verify that the API specification was retrieved and documented via show_api_doc(), (2) Confirm that all parameters match the specification exactly (names, types, required vs. optional fields), (3) Verify that all prerequisite data aggregation and validation steps have completed successfully, (4) Document the specification and parameter values explicitly in the execution log, (5) Only after all verification passes, execute the irreversible operation. This ensures that parameter mismatches or incomplete validation do not cause unintended state changes.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00275",
      "content": "[ver-00027] After completing a multi-phase workflow, verify that all intermediate variables were correctly created and persisted by: (1) Confirming that no NameError or undefined variable exceptions occurred during execution, (2) Spot-checking that variables referenced in later phases (e.g., access_token, filtered_items) were successfully instantiated in earlier phases, (3) If using multiple steps, confirming that variables output from step N are available as input to step N+1 through the step interface (not lost between steps).",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00284",
      "content": "[ver-00027] Before calling complete_task() on any state-modification workflow, verify that ALL intended modifications have been executed and persisted: (1) Re-query the affected resource(s) to confirm current state matches the target state, (2) If the task involves multiple independent operations (e.g., update one item AND disable multiple others), verify each operation independently, (3) Document the final verified state before proceeding to task completion, (4) Only call complete_task() after verification passes, not before.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00288",
      "content": "[ver-00027] When verifying extrema operations on items with aggregated sub-component metrics (e.g., longest playlist), verify by: (1) Spot-checking the aggregation calculation for the identified extremum item (e.g., manually summing song durations for the longest playlist), (2) Confirming that the aggregation includes all sub-items (e.g., all songs in the playlist were included in the duration sum), (3) Spot-checking the aggregation for a non-extremum item to ensure the aggregation logic is consistent, (4) Confirming the final formatting was applied correctly (e.g., rounding to nearest integer if required).",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00297",
      "content": "[ver-00027] After completing a multi-step workflow with sequential code execution: (1) Verify that each step's action field contained actual executable code (not pseudocode), (2) Confirm that output variables from each step were created and used in subsequent steps without NameError, (3) Verify that the terminal completion step (apis.supervisor.complete_task()) was executed with the final answer, (4) If task completion was marked null or incomplete, trace back to identify which action field was empty or contained non-executable content.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00301",
      "content": "[ver-00027] When completing multi-step workflows with extrema operations (finding min/max), verify not only that the correct item was identified but also that: (1) The enriched detail API (e.g., show_song()) was called for ALL items in the collection, not just a subset, (2) The extremum metric field (e.g., play_count) was present in all enriched responses, (3) The comparison function (min/max with key parameter) was applied to the complete enriched item list, (4) The primary action (e.g., play_music()) was executed with the correct identifier from the extremum item, (5) Post-action verification (e.g., show_current_song()) confirms the action succeeded on the intended item.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00306",
      "content": "[ver-00027] [ver-00025] When verifying action API calls (e.g., play_music(), send_payment(), create_item()): (1) Inspect the response object for success indicator fields (e.g., 'message', 'status', 'success', 'error'), (2) Confirm success messaging is present and contains expected keywords (e.g., 'now playing' for play_music, 'sent' for payments), (3) If available, call a follow-up status/query API to confirm the action persisted (e.g., show_current_song() after play_music()), (4) Only after both response inspection and optional state verification confirm success, proceed to task completion marking. This ensures the action actually executed, not just that the API accepted the request.",
      "helpful": 2,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00310",
      "content": "[ver-00042] After search/query operations that return multiple candidates, verify the target entity was correctly identified before attempting retrieval or dependent operations by: (1) Confirming the selected result's metadata (artist name, release date, genre) aligns with the task's implied intent or context, (2) If multiple plausible matches exist, documenting why one was selected over others, (3) Spot-checking that the selected entity's properties (e.g., artist name for an album) match expectations before proceeding to extraction or filtering operations on that entity's contents.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00314",
      "content": "[ver-00301] After executing a multi-step workflow, verify that: (1) All required variables were instantiated in 'action' fields and persisted across steps (confirm by checking variable references in downstream steps succeeded), (2) Each phase of the workflow completed successfully (check API response status codes, error fields, and counts at each checkpoint), (3) The final completion marking step (apis.supervisor.complete_task()) was executed and returned success, (4) The task's final_answer field is not null and contains a descriptive summary of actions taken. If final_answer is null or completion step was not executed, the task is incomplete.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00318",
      "content": "[ver-00027] Before calling any API with field access (e.g., response.get('key') or response['key']), verify the actual response structure by: (1) Calling apis.api_docs.show_api_doc() to inspect the schema, (2) If the API was successfully called in a prior step, cross-reference the actual response type from that step's output, (3) Confirming whether the response is a dict, list, or other type, (4) Using defensive coding (isinstance checks) if response structure is ambiguous. This prevents AttributeError/KeyError caused by incorrect field access assumptions.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00322",
      "content": "[ver-00027] When verifying bulk operations on qualifier-constrained tasks (e.g., 'accept requests from coworkers and friends'), confirm filtering correctness by: (1) Sampling accepted items and verifying that each sender/participant is in the target entity list (coworkers, friends, etc.), (2) Sampling rejected or non-accepted items and confirming that their senders/participants are NOT in the target entity list, (3) Documenting the total number of items in the full dataset, the number of target entities identified, the number of items after filtering, and the number of items operated on, (4) Confirming that the number of operated items matches the number of filtered items (not the total dataset size), ensuring filtering was applied before execution.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00328",
      "content": "[ver-00306] After calling irreversible or final API operations (task completion, account termination, data deletion): (1) Inspect the API response object for error fields (e.g., 'error', 'error_message', 'error_code'), (2) Check for status or success indicators (e.g., 'status' == 'success', 'success' == true), (3) Verify that critical output fields are populated (e.g., for task completion, verify final_answer is not null), (4) If the response indicates failure or missing critical fields, do not proceed; instead, investigate the cause and retry with corrected parameters, (5) Document the response structure and confirm it matches expectations before considering the task complete.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00332",
      "content": "[ver-00027] When verifying superlative results (most/least played, most/least liked, etc.), confirm pagination exhaustion by: (1) Documenting the total number of pages retrieved and total items accumulated across all pages, (2) Confirming that the final page returned fewer items than the page_limit, or was empty, or returned a null/empty next_page token, (3) Spot-checking that the identified superlative item (e.g., the song with highest play_count) is not suspiciously close to the pagination boundary (e.g., the last item on the final page), which could indicate incomplete pagination, (4) If applicable, re-querying with a different page_limit value to confirm the total item count remains consistent, ensuring no pages were skipped.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00335",
      "content": "[ver-00027] When verifying bulk follow/subscription operations, implement a formal specification-first checkpoint workflow: (1) Retrieve the show_api_doc() specification for the verification API (e.g., show_following_artists) to document the exact response structure and field names, (2) Execute the verification query with exhaustive pagination from the start (not just first page), (3) Extract identifiers using the correct field name from the spec, (4) Compare the complete extracted identifier set against the target identifier set, (5) Only after exhaustive verification passes do you confirm the operation is complete. This prevents silent field-name failures and incomplete pagination from producing false verification results.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00337",
      "content": "[ver-00027] When verifying bulk operations based on set difference logic (e.g., 'unfollow artists NOT in the liked_songs set'), verify BOTH directions of the set relationship: (1) Confirm that all items removed from the primary collection are NOT in the reference set (e.g., unfollowed artists have no liked songs), (2) Confirm that all remaining items in the primary collection ARE in the reference set (e.g., all remaining followed artists have at least one liked song). This bidirectional verification prevents logical errors where the set difference computation may be correct but the reference set itself is incomplete or incorrect.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00341",
      "content": "[ver-00027] [ver-00025] When verifying bulk follow/relationship operations, confirm that field names in the verification API response are correct by: (1) Consulting show_api_doc() for the verification API (e.g., show_following_artists) BEFORE writing verification code, (2) Documenting the exact field name that contains the entity identifier (e.g., 'artist_id' not 'id'), (3) Verifying that field access returns non-None values for a sample of items before proceeding to bulk verification, (4) If verification reports 0 items or all items as missing, immediately re-check the field name specification as this often indicates silent field access failures (None values) due to incorrect field names.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00343",
      "content": "[ver-00027] [ver-00025] When verifying that bulk operations persisted after execution, re-query using identical filter criteria to the original operation. For multi-participant filtering tasks, this means re-querying with the same OR logic across the same participant fields and the same target entity list. This ensures verification covers the exact same data subset that was operated on, preventing false negatives from filter misalignment.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00348",
      "content": "[ver-00027] After completing a multi-phase workflow, verify execution completeness by: (1) Checking that the 'observation' field shows actual execution results (API responses, file creation confirmation, variable values printed), not just the code itself, (2) Confirming that all side effects occurred (files created, APIs called, accounts modified), (3) Verifying that final_answer is populated and not null, indicating task completion APIs executed, (4) If 'observation' is empty or shows only pseudocode without execution output, the code was placed in 'thought' instead of 'action' and did not execute. Re-execute the code in the 'action' field.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00351",
      "content": "[ver-00XXX] After executing a multi-phase workflow, verify actual code execution by: (1) Checking the observation output for concrete results (API response data, variable values, file creation confirmations) rather than just the presence of code in thought fields, (2) Confirming that final_answer is not null and contains the expected supervisor.complete_task() output, indicating the irreversible operation was actually executed, (3) Spot-checking the observation for phase-specific outputs (e.g., 'Retrieved X songs from library', 'Deduplicated to Y unique songs', 'File created at path Z') to confirm each phase completed, (4) If observation shows 'No code to execute', immediately check that the 'action' field contains code; if empty, the task has not been executed and must be resubmitted with code in the action field.",
      "helpful": 1,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00354",
      "content": "[ver-00027] When calling final/irreversible APIs (task completion, account changes, deletions): (1) Verify the API response includes success indicators (e.g., 'Marked complete', 'Success: true'), (2) Verify that critical output fields from the specification are populated in the response (e.g., if the spec indicates 'final_answer' should be returned, confirm it is present and matches the input), (3) If critical output fields are missing or null, do NOT assume the operation succeeded—re-query or inspect system state to confirm the operation persisted correctly, (4) Document both the success message AND the critical output fields to confirm the operation registered completely.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00361",
      "content": "[ver-00027] [ver-00025] Before calling apis.supervisor.complete_task(), verify that: (1) All data extraction steps have executed successfully (confirmed by non-empty observations), (2) The final answer has been calculated and is in the correct format (e.g., numeric value for streak length, string for names), (3) The answer matches the task requirements and is derived from the complete, paginated dataset (not a partial result), (4) All intermediate steps were executed in the 'action' field, not the 'thought' field.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00369",
      "content": "[ver-00027] When extracting data from unstructured note content using string parsing: (1) Verify that the parsing logic correctly identifies all relevant data entries (e.g., all habit streak records) by checking the total count of parsed items matches the expected structure. (2) Spot-check a sample of parsed items to confirm the regex or split logic extracted the correct values. (3) Verify that the extrema function (max, min) was applied to the complete parsed dataset, not a subset. (4) Confirm that the final answer extracted from the parsed data is a valid value (e.g., not null, not a string when a number is expected) before submitting.",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00371",
      "content": "[ver-00027] After approving payment requests in bulk, verify approval persistence by: (1) Re-querying apis.venmo.get_payment_requests() to confirm that previously pending requests are no longer in the pending state, (2) Optionally querying apis.venmo.get_approved_requests() or similar to confirm approved requests now appear in the approved/completed state, (3) Sampling at least 2-3 of the approved requests to confirm their status field reflects the approval, (4) Confirming that requests from non-target contacts remain in pending state (unchanged).",
      "helpful": 0,
      "harmful": 0,
      "section": "verification_checklist"
    },
    {
      "id": "ver-00375",
      "content": "[ver-00027] After calling the completion API (e.g., apis.supervisor.complete_task()), verify task completion status by: (1) Inspecting the response object for success indicators (e.g., 'message' field, status code), (2) Calling apis.supervisor.show_active_task() to confirm the active task is None or has status='complete', (3) If verification shows the task is still incomplete, log the response details and investigate whether a syntax error or API parameter mismatch caused the failure. This verification step prevents silent failures where the completion call fails but prior operations (e.g., approvals) succeeded, leaving the task incomplete in the system.",
      "helpful": 0,
      "harmful": 0
    }
  ]
}